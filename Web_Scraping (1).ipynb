{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scraping",
      "provenance": [],
      "collapsed_sections": [
        "qOsWaxcH2Sqj",
        "dGYFV3NoaiAh",
        "k-54Tu_HKdhL",
        "DuaF3cLIOdGV",
        "OQeAVtWCYqq_",
        "ArEFuxP_tDPB",
        "GQYaxqZmyMF0",
        "oE7WE0Jymi09",
        "djadVOZYqAIK",
        "8ybBqQ9gt188",
        "VCGjCDaKhvwn",
        "JbgXefmfjKQT",
        "6evlpiLSkFSd",
        "JHfPtChsrlh6",
        "SBk8Dx5gs_YF",
        "EK0s3B84t_TB",
        "fMLI2Rc5vIFa",
        "hDzZBjkVwI-S",
        "G7axVVurxoTM",
        "NRP0gmE0ymDk",
        "M8SDypRyARXf",
        "pUO54SYJBhbK",
        "YWJCsxNbCcAF",
        "Hh-tb49vDs4Q",
        "M6o1BHhDcxvr",
        "W6dHlkRm8skI",
        "ogdJ24dK9Eor",
        "_lFGi97S9jzo",
        "cuJErwFB9xxo",
        "q10c_IqC98Gy",
        "dDs7_dii-BTQ",
        "2M8Cfl9A-HnB",
        "LP1nXl1C-QqP",
        "6sEsgVM3-WbX",
        "QaVU9AKf-dHf",
        "ES5kCab5-iE_",
        "dCVW9lZ3-j5n",
        "_h7oZX2T-q9S",
        "QSVEkK1Z-vKP",
        "AMFDhTvz-zUP",
        "Q01Kr9Ja-2bH",
        "sax9ArgV-9mg",
        "3CB2-ESe_KPU",
        "LA_xgQKS_PqX",
        "9li8yWLI_TsP",
        "-pMXrAqn_YNd",
        "XIIFrCwd_e_I",
        "HDyCmTv3_kIv",
        "NpJNf0IA_nUz",
        "yh8kvkm9gz3_",
        "kJVppA3RhObK",
        "ykUXisSFjxsD",
        "UDOwMrm8jzEo",
        "75ml_fUaj1o4",
        "DvfCKO4uj3hw",
        "p4EIZhaCj8iS",
        "BpDJDzidj-Mh",
        "pWqAsXyKj_hQ",
        "juiE_TockBSY",
        "53D9-1GAkCnR",
        "SrGpHLtVkD7Y",
        "leI85u_nkFRQ",
        "L5Fh6f6dkG1o",
        "SYhzZ8bkkIWI",
        "KvhHDmLFkJpZ",
        "4HzNKI3TkLOI",
        "FpV0ziyQkMpp",
        "3cmzmpUtkOUQ",
        "FO8miYNhkQRw",
        "DnP_S7_CkRlP",
        "E7WR_0T2kTL4",
        "cD_niBeTkUlH",
        "QOSRwP_CkVtn",
        "Nd5Js-RMkXNI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOsWaxcH2Sqj"
      },
      "source": [
        "# Import necessary packages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlKygQuDDCGg"
      },
      "source": [
        "Read in reviews using BeautifulSoup and then reduced it so we just have the review and the rating. The following code was used to scrape the code: https://gist.github.com/adelweiss/3c13890420662b5041210c48792b4764#file-metacritic-web-scraper . See bottom of the notebook for how the code was edited so the algorithm could be used for my own efficient scraping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0qDWPSfDOw-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "779a63fd-4a82-4f37-c688-07b25371fcbe"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import threading\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGYFV3NoaiAh"
      },
      "source": [
        "# Test Using Old vs New Scraper "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQUyAk0PkWgK"
      },
      "source": [
        "We want to look how long it takes to run the scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR4bNDe6kS1r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "440e7f45-1003-400e-e154-a9372c6b4fed"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n",
            "Building wheels for collected packages: ipython-autotime\n",
            "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=646dbc256474e5e9c76828cb1e230fd092886970b452363a9feb1ae86b03b889\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n",
            "Successfully built ipython-autotime\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ_4btu2ch7L"
      },
      "source": [
        "The following fails if we go over too many pages. This line:\n",
        "\n",
        "\n",
        "*   review.find('div', class_='review_body').find('span').text) \n",
        "*   Gives the following error: 'NoneType' object has no attribute 'text'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mdJZn_GYqJj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "a09aa0da-461f-4ab0-f016-3d61868a1066"
      },
      "source": [
        "review_dict = {'name':[], 'date':[], 'rating':[], 'review':[]}\n",
        "\n",
        "for page in range(0,20):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-last-of-us-part-ii/user-reviews?page='+str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response  = requests.get(url, headers = user_agent)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "                       break \n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(review.find('span', class_='blurb blurb_expanded').text) # if users need to click expand (longer reviews)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text) # for normal sized/shorter reviews  \n",
        "                                                                                         \n",
        "    df = pd.DataFrame(review_dict)\n",
        "    df = df.drop(columns=['name', 'date'])\n",
        "    df.to_csv(r\"drive/My Drive/csv's/Test_old.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-99ff3bd0e1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             review_dict['review'].append(\n\u001b[0;32m---> 18\u001b[0;31m                 review.find('div', class_='review_body').find('span').text) \n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNpOrDcGbNqR"
      },
      "source": [
        "1) Change to code 1 - Successfully removes error and runs for desired number of pages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD9N0YLMd6zO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2eab5ba2-5f23-4ba8-da15-9e85ce127299"
      },
      "source": [
        "  review_dict = {'name':[], 'date':[], 'rating':[], 'review':[]}\n",
        "\n",
        "for page in range(0,300):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-last-of-us-part-ii/user-reviews?page='+str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response  = requests.get(url, headers = user_agent)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "                       break \n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(review.find('span', class_='blurb blurb_expanded').text \n",
        "            if review.find('span', class_='blurb blurb_expanded')\n",
        "            else None\n",
        "            )\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text \n",
        "                if review.find('div', class_='review_body').find('span') # Adding this if/else removes NoneType error  \n",
        "                else None # if there is no text attribute do nothing \n",
        "                ) \n",
        "            \n",
        "    df = pd.DataFrame(review_dict)\n",
        "    df = df.drop(columns=['name', 'date'])\n",
        "    df.to_csv(r\"drive/My Drive/csv's/Test_New1.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWVi62QNhTUF"
      },
      "source": [
        "2) Change to code 2 - Add threads to speed up process of scraping. For the same number of pages, it reduces the time taken from **8 minutes 59 seconds** to **2 minutes 11 seconds**. This **saves 6 minutes 48 seconds**. This reduces the time of scraping by approximately 76%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugSVIB_ahnJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6b3eaa7-3db8-4156-c717-59585cd31670"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-last-of-us-part-ii/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(review.find('span', class_='blurb blurb_expanded').text \n",
        "            if review.find('span', class_='blurb blurb_expanded')\n",
        "            else None\n",
        "            )\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text \n",
        "                if review.find('div',class_='review_body').find('span') \n",
        "                else None\n",
        "                )\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,)) # uses the scrap_page method above\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "\n",
        "df = pd.DataFrame(review_dict)\n",
        "df = df.drop(columns=['name', 'date'])\n",
        "df.to_csv(r\"drive/My Drive/csv's/Test_New2.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(300), 100)\n",
        "    # chunkIt(range(10), 3) ---> [[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]] this would be approx 3 threads\n",
        "    # https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length/2130035#2130035 for help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n",
            "[*] Working on page: 42\n",
            "[*] Working on page: 43\n",
            "[*] Working on page: 44\n",
            "[*] Working on page: 45\n",
            "[*] Working on page: 46\n",
            "[*] Working on page: 47\n",
            "[*] Working on page: 48\n",
            "[*] Working on page: 49\n",
            "[*] Working on page: 50\n",
            "[*] Working on page: 51\n",
            "[*] Working on page: 52\n",
            "[*] Working on page: 53\n",
            "[*] Working on page: 54\n",
            "[*] Working on page: 55\n",
            "[*] Working on page: 56\n",
            "[*] Working on page: 57\n",
            "[*] Working on page: 58\n",
            "[*] Working on page: 59\n",
            "[*] Working on page: 60\n",
            "[*] Working on page: 61\n",
            "[*] Working on page: 62\n",
            "[*] Working on page: 63\n",
            "[*] Working on page: 64\n",
            "[*] Working on page: 65\n",
            "[*] Working on page: 66\n",
            "[*] Working on page: 67\n",
            "[*] Working on page: 68\n",
            "[*] Working on page: 69\n",
            "[*] Working on page: 70\n",
            "[*] Working on page: 71\n",
            "[*] Working on page: 72\n",
            "[*] Working on page: 73\n",
            "[*] Working on page: 74\n",
            "[*] Working on page: 75\n",
            "[*] Working on page: 76\n",
            "[*] Working on page: 77\n",
            "[*] Working on page: 78\n",
            "[*] Working on page: 79\n",
            "[*] Working on page: 80\n",
            "[*] Working on page: 81\n",
            "[*] Working on page: 82\n",
            "[*] Working on page: 83\n",
            "[*] Working on page: 84\n",
            "[*] Working on page: 85\n",
            "[*] Working on page: 86\n",
            "[*] Working on page: 87\n",
            "[*] Working on page: 88\n",
            "[*] Working on page: 89\n",
            "[*] Working on page: 90\n",
            "[*] Working on page: 91\n",
            "[*] Working on page: 92\n",
            "[*] Working on page: 93\n",
            "[*] Working on page: 94\n",
            "[*] Working on page: 95\n",
            "[*] Working on page: 96\n",
            "[*] Working on page: 97\n",
            "[*] Working on page: 98\n",
            "[*] Working on page: 99\n",
            "[*] Working on page: 100\n",
            "[*] Working on page: 101\n",
            "[*] Working on page: 102\n",
            "[*] Working on page: 103\n",
            "[*] Working on page: 104\n",
            "[*] Working on page: 105\n",
            "[*] Working on page: 106\n",
            "[*] Working on page: 107\n",
            "[*] Working on page: 108\n",
            "[*] Working on page: 109\n",
            "[*] Working on page: 110\n",
            "[*] Working on page: 111\n",
            "[*] Working on page: 112\n",
            "[*] Working on page: 113\n",
            "[*] Working on page: 114\n",
            "[*] Working on page: 115\n",
            "[*] Working on page: 116\n",
            "[*] Working on page: 117\n",
            "[*] Working on page: 118\n",
            "[*] Working on page: 119\n",
            "[*] Working on page: 120\n",
            "[*] Working on page: 121\n",
            "[*] Working on page: 122\n",
            "[*] Working on page: 123\n",
            "[*] Working on page: 124\n",
            "[*] Working on page: 125\n",
            "[*] Working on page: 126\n",
            "[*] Working on page: 127\n",
            "[*] Working on page: 128\n",
            "[*] Working on page: 129\n",
            "[*] Working on page: 130\n",
            "[*] Working on page: 131\n",
            "[*] Working on page: 132\n",
            "[*] Working on page: 133\n",
            "[*] Working on page: 134\n",
            "[*] Working on page: 135\n",
            "[*] Working on page: 136\n",
            "[*] Working on page: 137\n",
            "[*] Working on page: 138\n",
            "[*] Working on page: 139\n",
            "[*] Working on page: 140\n",
            "[*] Working on page: 141\n",
            "[*] Working on page: 142\n",
            "[*] Working on page: 143\n",
            "[*] Working on page: 144\n",
            "[*] Working on page: 145\n",
            "[*] Working on page: 146\n",
            "[*] Working on page: 147\n",
            "[*] Working on page: 148\n",
            "[*] Working on page: 149\n",
            "[*] Working on page: 150\n",
            "[*] Working on page: 151\n",
            "[*] Working on page: 152\n",
            "[*] Working on page: 153\n",
            "[*] Working on page: 154\n",
            "[*] Working on page: 155\n",
            "[*] Working on page: 156\n",
            "[*] Working on page: 157\n",
            "[*] Working on page: 158\n",
            "[*] Working on page: 159\n",
            "[*] Working on page: 160\n",
            "[*] Working on page: 161\n",
            "[*] Working on page: 162\n",
            "[*] Working on page: 163\n",
            "[*] Working on page: 164\n",
            "[*] Working on page: 165\n",
            "[*] Working on page: 166\n",
            "[*] Working on page: 167\n",
            "[*] Working on page: 168\n",
            "[*] Working on page: 169\n",
            "[*] Working on page: 170\n",
            "[*] Working on page: 171\n",
            "[*] Working on page: 172\n",
            "[*] Working on page: 173\n",
            "[*] Working on page: 174\n",
            "[*] Working on page: 175\n",
            "[*] Working on page: 176\n",
            "[*] Working on page: 177\n",
            "[*] Working on page: 178\n",
            "[*] Working on page: 179\n",
            "[*] Working on page: 180\n",
            "[*] Working on page: 181\n",
            "[*] Working on page: 182\n",
            "[*] Working on page: 183\n",
            "[*] Working on page: 184\n",
            "[*] Working on page: 185\n",
            "[*] Working on page: 186\n",
            "[*] Working on page: 187\n",
            "[*] Working on page: 188\n",
            "[*] Working on page: 189\n",
            "[*] Working on page: 190\n",
            "[*] Working on page: 191\n",
            "[*] Working on page: 192\n",
            "[*] Working on page: 193\n",
            "[*] Working on page: 194\n",
            "[*] Working on page: 195\n",
            "[*] Working on page: 196\n",
            "[*] Working on page: 197\n",
            "[*] Working on page: 198\n",
            "[*] Working on page: 199\n",
            "[*] Working on page: 200\n",
            "[*] Working on page: 201\n",
            "[*] Working on page: 202\n",
            "[*] Working on page: 203\n",
            "[*] Working on page: 204\n",
            "[*] Working on page: 205\n",
            "[*] Working on page: 206\n",
            "[*] Working on page: 207\n",
            "[*] Working on page: 208\n",
            "[*] Working on page: 209\n",
            "[*] Working on page: 210\n",
            "[*] Working on page: 211\n",
            "[*] Working on page: 212\n",
            "[*] Working on page: 213\n",
            "[*] Working on page: 214\n",
            "[*] Working on page: 215\n",
            "[*] Working on page: 216\n",
            "[*] Working on page: 217\n",
            "[*] Working on page: 218\n",
            "[*] Working on page: 219\n",
            "[*] Working on page: 220\n",
            "[*] Working on page: 221\n",
            "[*] Working on page: 222\n",
            "[*] Working on page: 223\n",
            "[*] Working on page: 224\n",
            "[*] Working on page: 225\n",
            "[*] Working on page: 226\n",
            "[*] Working on page: 227\n",
            "[*] Working on page: 228\n",
            "[*] Working on page: 229\n",
            "[*] Working on page: 230\n",
            "[*] Working on page: 231\n",
            "[*] Working on page: 232\n",
            "[*] Working on page: 233\n",
            "[*] Working on page: 234\n",
            "[*] Working on page: 235\n",
            "[*] Working on page: 236\n",
            "[*] Working on page: 237\n",
            "[*] Working on page: 238\n",
            "[*] Working on page: 239\n",
            "[*] Working on page: 240\n",
            "[*] Working on page: 241\n",
            "[*] Working on page: 242\n",
            "[*] Working on page: 243\n",
            "[*] Working on page: 244\n",
            "[*] Working on page: 245\n",
            "[*] Working on page: 246\n",
            "[*] Working on page: 247\n",
            "[*] Working on page: 248\n",
            "[*] Working on page: 249\n",
            "[*] Working on page: 250\n",
            "[*] Working on page: 251\n",
            "[*] Working on page: 252\n",
            "[*] Working on page: 253\n",
            "[*] Working on page: 254\n",
            "[*] Working on page: 255\n",
            "[*] Working on page: 256\n",
            "[*] Working on page: 257\n",
            "[*] Working on page: 258\n",
            "[*] Working on page: 259\n",
            "[*] Working on page: 260\n",
            "[*] Working on page: 261\n",
            "[*] Working on page: 262\n",
            "[*] Working on page: 263\n",
            "[*] Working on page: 264\n",
            "[*] Working on page: 265\n",
            "[*] Working on page: 266\n",
            "[*] Working on page: 267\n",
            "[*] Working on page: 268\n",
            "[*] Working on page: 269\n",
            "[*] Working on page: 270\n",
            "[*] Working on page: 271\n",
            "[*] Working on page: 272\n",
            "[*] Working on page: 273\n",
            "[*] Working on page: 274\n",
            "[*] Working on page: 275\n",
            "[*] Working on page: 276\n",
            "[*] Working on page: 277\n",
            "[*] Working on page: 278\n",
            "[*] Working on page: 279\n",
            "[*] Working on page: 280\n",
            "[*] Working on page: 281\n",
            "[*] Working on page: 282\n",
            "[*] Working on page: 283\n",
            "[*] Working on page: 284\n",
            "[*] Working on page: 285\n",
            "[*] Working on page: 286\n",
            "[*] Working on page: 287\n",
            "[*] Working on page: 288\n",
            "[*] Working on page: 289\n",
            "[*] Working on page: 290\n",
            "[*] Working on page: 291\n",
            "[*] Working on page: 292\n",
            "[*] Working on page: 293\n",
            "[*] Working on page: 294\n",
            "[*] Working on page: 295\n",
            "[*] Working on page: 296\n",
            "[*] Working on page: 297\n",
            "[*] Working on page: 298\n",
            "[*] Working on page: 299\n",
            "time: 2min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM2nJ8mvDe6f"
      },
      "source": [
        "# **Metacritic Datasets:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-54Tu_HKdhL"
      },
      "source": [
        "# 1) TLOU2 Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4S2NmpHKe7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b18758e-2729-41ed-f935-9088dc6503e7"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-last-of-us-part-ii/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span', class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div', class_='review_body').find('span') else None)\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0                                          #\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df1 = pd.DataFrame(review_dict)\n",
        "    df1 = df1.drop(columns=['name', 'date'])\n",
        "    df1.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/TLOU2_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # First argument is the number of pages + 1, \n",
        "    # and second argument is the chunk size, so the threads here will be 513 / 100 = 5 ( almost ) \n",
        " \n",
        "    \n",
        "    chunkIt(range(512), 100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n",
            "[*] Working on page: 42\n",
            "[*] Working on page: 43\n",
            "[*] Working on page: 44\n",
            "[*] Working on page: 45\n",
            "[*] Working on page: 46\n",
            "[*] Working on page: 47\n",
            "[*] Working on page: 48\n",
            "[*] Working on page: 49\n",
            "[*] Working on page: 50\n",
            "[*] Working on page: 51\n",
            "[*] Working on page: 52\n",
            "[*] Working on page: 53\n",
            "[*] Working on page: 54\n",
            "[*] Working on page: 55\n",
            "[*] Working on page: 56\n",
            "[*] Working on page: 57\n",
            "[*] Working on page: 58\n",
            "[*] Working on page: 59\n",
            "[*] Working on page: 60\n",
            "[*] Working on page: 61\n",
            "[*] Working on page: 62\n",
            "[*] Working on page: 63\n",
            "[*] Working on page: 64\n",
            "[*] Working on page: 65\n",
            "[*] Working on page: 66\n",
            "[*] Working on page: 67\n",
            "[*] Working on page: 68\n",
            "[*] Working on page: 69\n",
            "[*] Working on page: 70\n",
            "[*] Working on page: 71\n",
            "[*] Working on page: 72\n",
            "[*] Working on page: 73\n",
            "[*] Working on page: 74\n",
            "[*] Working on page: 75\n",
            "[*] Working on page: 76\n",
            "[*] Working on page: 77\n",
            "[*] Working on page: 78\n",
            "[*] Working on page: 79\n",
            "[*] Working on page: 80\n",
            "[*] Working on page: 81\n",
            "[*] Working on page: 82\n",
            "[*] Working on page: 83\n",
            "[*] Working on page: 84\n",
            "[*] Working on page: 85\n",
            "[*] Working on page: 86\n",
            "[*] Working on page: 87\n",
            "[*] Working on page: 88\n",
            "[*] Working on page: 89\n",
            "[*] Working on page: 90\n",
            "[*] Working on page: 91\n",
            "[*] Working on page: 92\n",
            "[*] Working on page: 93\n",
            "[*] Working on page: 94\n",
            "[*] Working on page: 95\n",
            "[*] Working on page: 96\n",
            "[*] Working on page: 97\n",
            "[*] Working on page: 98\n",
            "[*] Working on page: 99\n",
            "[*] Working on page: 100\n",
            "[*] Working on page: 101\n",
            "[*] Working on page: 102\n",
            "[*] Working on page: 103\n",
            "[*] Working on page: 104\n",
            "[*] Working on page: 105\n",
            "[*] Working on page: 106\n",
            "[*] Working on page: 107\n",
            "[*] Working on page: 108\n",
            "[*] Working on page: 109\n",
            "[*] Working on page: 110\n",
            "[*] Working on page: 111\n",
            "[*] Working on page: 112\n",
            "[*] Working on page: 113\n",
            "[*] Working on page: 114\n",
            "[*] Working on page: 115\n",
            "[*] Working on page: 116\n",
            "[*] Working on page: 117\n",
            "[*] Working on page: 118\n",
            "[*] Working on page: 119\n",
            "[*] Working on page: 120\n",
            "[*] Working on page: 121\n",
            "[*] Working on page: 122\n",
            "[*] Working on page: 123\n",
            "[*] Working on page: 124\n",
            "[*] Working on page: 125\n",
            "[*] Working on page: 126\n",
            "[*] Working on page: 127\n",
            "[*] Working on page: 128\n",
            "[*] Working on page: 129\n",
            "[*] Working on page: 130\n",
            "[*] Working on page: 131\n",
            "[*] Working on page: 132\n",
            "[*] Working on page: 133\n",
            "[*] Working on page: 134\n",
            "[*] Working on page: 135\n",
            "[*] Working on page: 136\n",
            "[*] Working on page: 137\n",
            "[*] Working on page: 138\n",
            "[*] Working on page: 139\n",
            "[*] Working on page: 140\n",
            "[*] Working on page: 141\n",
            "[*] Working on page: 142\n",
            "[*] Working on page: 143\n",
            "[*] Working on page: 144\n",
            "[*] Working on page: 145\n",
            "[*] Working on page: 146\n",
            "[*] Working on page: 147\n",
            "[*] Working on page: 148\n",
            "[*] Working on page: 149\n",
            "[*] Working on page: 150\n",
            "[*] Working on page: 151\n",
            "[*] Working on page: 152\n",
            "[*] Working on page: 153\n",
            "[*] Working on page: 154\n",
            "[*] Working on page: 155\n",
            "[*] Working on page: 156\n",
            "[*] Working on page: 157\n",
            "[*] Working on page: 158\n",
            "[*] Working on page: 159\n",
            "[*] Working on page: 160\n",
            "[*] Working on page: 161\n",
            "[*] Working on page: 162\n",
            "[*] Working on page: 163\n",
            "[*] Working on page: 164\n",
            "[*] Working on page: 165\n",
            "[*] Working on page: 166\n",
            "[*] Working on page: 167\n",
            "[*] Working on page: 168\n",
            "[*] Working on page: 169\n",
            "[*] Working on page: 170\n",
            "[*] Working on page: 171\n",
            "[*] Working on page: 172\n",
            "[*] Working on page: 173\n",
            "[*] Working on page: 174\n",
            "[*] Working on page: 175\n",
            "[*] Working on page: 176\n",
            "[*] Working on page: 177\n",
            "[*] Working on page: 178\n",
            "[*] Working on page: 179\n",
            "[*] Working on page: 180\n",
            "[*] Working on page: 181\n",
            "[*] Working on page: 182\n",
            "[*] Working on page: 183\n",
            "[*] Working on page: 184\n",
            "[*] Working on page: 185\n",
            "[*] Working on page: 186\n",
            "[*] Working on page: 187\n",
            "[*] Working on page: 188\n",
            "[*] Working on page: 189\n",
            "[*] Working on page: 190\n",
            "[*] Working on page: 191\n",
            "[*] Working on page: 192\n",
            "[*] Working on page: 193\n",
            "[*] Working on page: 194\n",
            "[*] Working on page: 195\n",
            "[*] Working on page: 196\n",
            "[*] Working on page: 197\n",
            "[*] Working on page: 198\n",
            "[*] Working on page: 199\n",
            "[*] Working on page: 200\n",
            "[*] Working on page: 201\n",
            "[*] Working on page: 202\n",
            "[*] Working on page: 203\n",
            "[*] Working on page: 204\n",
            "[*] Working on page: 205\n",
            "[*] Working on page: 206\n",
            "[*] Working on page: 207\n",
            "[*] Working on page: 208\n",
            "[*] Working on page: 209\n",
            "[*] Working on page: 210\n",
            "[*] Working on page: 211\n",
            "[*] Working on page: 212\n",
            "[*] Working on page: 213\n",
            "[*] Working on page: 214\n",
            "[*] Working on page: 215\n",
            "[*] Working on page: 216\n",
            "[*] Working on page: 217\n",
            "[*] Working on page: 218\n",
            "[*] Working on page: 219\n",
            "[*] Working on page: 220\n",
            "[*] Working on page: 221\n",
            "[*] Working on page: 222\n",
            "[*] Working on page: 223\n",
            "[*] Working on page: 224\n",
            "[*] Working on page: 225\n",
            "[*] Working on page: 226\n",
            "[*] Working on page: 227\n",
            "[*] Working on page: 228\n",
            "[*] Working on page: 229\n",
            "[*] Working on page: 230\n",
            "[*] Working on page: 231\n",
            "[*] Working on page: 232\n",
            "[*] Working on page: 233\n",
            "[*] Working on page: 234\n",
            "[*] Working on page: 235\n",
            "[*] Working on page: 236\n",
            "[*] Working on page: 237\n",
            "[*] Working on page: 238\n",
            "[*] Working on page: 239\n",
            "[*] Working on page: 240\n",
            "[*] Working on page: 241\n",
            "[*] Working on page: 242\n",
            "[*] Working on page: 243\n",
            "[*] Working on page: 244\n",
            "[*] Working on page: 245\n",
            "[*] Working on page: 246\n",
            "[*] Working on page: 247\n",
            "[*] Working on page: 248\n",
            "[*] Working on page: 249\n",
            "[*] Working on page: 250\n",
            "[*] Working on page: 251\n",
            "[*] Working on page: 252\n",
            "[*] Working on page: 253\n",
            "[*] Working on page: 254\n",
            "[*] Working on page: 255\n",
            "[*] Working on page: 256\n",
            "[*] Working on page: 257\n",
            "[*] Working on page: 258\n",
            "[*] Working on page: 259\n",
            "[*] Working on page: 260\n",
            "[*] Working on page: 261\n",
            "[*] Working on page: 262\n",
            "[*] Working on page: 263\n",
            "[*] Working on page: 264\n",
            "[*] Working on page: 265\n",
            "[*] Working on page: 266\n",
            "[*] Working on page: 267\n",
            "[*] Working on page: 268\n",
            "[*] Working on page: 269\n",
            "[*] Working on page: 270\n",
            "[*] Working on page: 271\n",
            "[*] Working on page: 272\n",
            "[*] Working on page: 273\n",
            "[*] Working on page: 274\n",
            "[*] Working on page: 275\n",
            "[*] Working on page: 276\n",
            "[*] Working on page: 277\n",
            "[*] Working on page: 278\n",
            "[*] Working on page: 279\n",
            "[*] Working on page: 280\n",
            "[*] Working on page: 281\n",
            "[*] Working on page: 282\n",
            "[*] Working on page: 283\n",
            "[*] Working on page: 284\n",
            "[*] Working on page: 285\n",
            "[*] Working on page: 286\n",
            "[*] Working on page: 287\n",
            "[*] Working on page: 288\n",
            "[*] Working on page: 289\n",
            "[*] Working on page: 290\n",
            "[*] Working on page: 291\n",
            "[*] Working on page: 292\n",
            "[*] Working on page: 293\n",
            "[*] Working on page: 294\n",
            "[*] Working on page: 295\n",
            "[*] Working on page: 296\n",
            "[*] Working on page: 297\n",
            "[*] Working on page: 298\n",
            "[*] Working on page: 299\n",
            "[*] Working on page: 300\n",
            "[*] Working on page: 301\n",
            "[*] Working on page: 302\n",
            "[*] Working on page: 303\n",
            "[*] Working on page: 304\n",
            "[*] Working on page: 305\n",
            "[*] Working on page: 306\n",
            "[*] Working on page: 307\n",
            "[*] Working on page: 308\n",
            "[*] Working on page: 309\n",
            "[*] Working on page: 310\n",
            "[*] Working on page: 311\n",
            "[*] Working on page: 312\n",
            "[*] Working on page: 313\n",
            "[*] Working on page: 314\n",
            "[*] Working on page: 315\n",
            "[*] Working on page: 316\n",
            "[*] Working on page: 317\n",
            "[*] Working on page: 318\n",
            "[*] Working on page: 319\n",
            "[*] Working on page: 320\n",
            "[*] Working on page: 321\n",
            "[*] Working on page: 322\n",
            "[*] Working on page: 323\n",
            "[*] Working on page: 324\n",
            "[*] Working on page: 325\n",
            "[*] Working on page: 326\n",
            "[*] Working on page: 327\n",
            "[*] Working on page: 328\n",
            "[*] Working on page: 329\n",
            "[*] Working on page: 330\n",
            "[*] Working on page: 331\n",
            "[*] Working on page: 332\n",
            "[*] Working on page: 333\n",
            "[*] Working on page: 334\n",
            "[*] Working on page: 335\n",
            "[*] Working on page: 336\n",
            "[*] Working on page: 337\n",
            "[*] Working on page: 338\n",
            "[*] Working on page: 339\n",
            "[*] Working on page: 340\n",
            "[*] Working on page: 341\n",
            "[*] Working on page: 342\n",
            "[*] Working on page: 343\n",
            "[*] Working on page: 344\n",
            "[*] Working on page: 345\n",
            "[*] Working on page: 346\n",
            "[*] Working on page: 347\n",
            "[*] Working on page: 348\n",
            "[*] Working on page: 349\n",
            "[*] Working on page: 350\n",
            "[*] Working on page: 351\n",
            "[*] Working on page: 352\n",
            "[*] Working on page: 353\n",
            "[*] Working on page: 354\n",
            "[*] Working on page: 355\n",
            "[*] Working on page: 356\n",
            "[*] Working on page: 357\n",
            "[*] Working on page: 358\n",
            "[*] Working on page: 359\n",
            "[*] Working on page: 360\n",
            "[*] Working on page: 361\n",
            "[*] Working on page: 362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-367:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 159, in _new_conn\n",
            "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\", line 80, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\", line 70, in create_connection\n",
            "    sock.connect(sa)\n",
            "TimeoutError: [Errno 110] Connection timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
            "    chunked=chunked)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 343, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 839, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 301, in connect\n",
            "    conn = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 168, in _new_conn\n",
            "    self, \"Failed to establish a new connection: %s\" % e)\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f7bffb59358>: Failed to establish a new connection: [Errno 110] Connection timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
            "    _stacktrace=sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\", line 399, in increment\n",
            "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
            "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.metacritic.com', port=443): Max retries exceeded with url: /game/playstation-4/the-last-of-us-part-ii/user-reviews?page=362 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f7bffb59358>: Failed to establish a new connection: [Errno 110] Connection timed out',))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-2-62e84cacedd1>\", line 5, in scrap_page\n",
            "    response = requests.get(url, headers=user_agent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 76, in get\n",
            "    return request('get', url, params=params, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 61, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 516, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.metacritic.com', port=443): Max retries exceeded with url: /game/playstation-4/the-last-of-us-part-ii/user-reviews?page=362 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f7bffb59358>: Failed to establish a new connection: [Errno 110] Connection timed out',))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 363\n",
            "[*] Working on page: 364\n",
            "[*] Working on page: 365\n",
            "[*] Working on page: 366\n",
            "[*] Working on page: 367\n",
            "[*] Working on page: 368\n",
            "[*] Working on page: 369\n",
            "[*] Working on page: 370\n",
            "[*] Working on page: 371\n",
            "[*] Working on page: 372\n",
            "[*] Working on page: 373\n",
            "[*] Working on page: 374\n",
            "[*] Working on page: 375\n",
            "[*] Working on page: 376\n",
            "[*] Working on page: 377\n",
            "[*] Working on page: 378\n",
            "[*] Working on page: 379\n",
            "[*] Working on page: 380\n",
            "[*] Working on page: 381\n",
            "[*] Working on page: 382\n",
            "[*] Working on page: 383\n",
            "[*] Working on page: 384\n",
            "[*] Working on page: 385\n",
            "[*] Working on page: 386\n",
            "[*] Working on page: 387\n",
            "[*] Working on page: 388\n",
            "[*] Working on page: 389\n",
            "[*] Working on page: 390\n",
            "[*] Working on page: 391\n",
            "[*] Working on page: 392\n",
            "[*] Working on page: 393\n",
            "[*] Working on page: 394\n",
            "[*] Working on page: 395\n",
            "[*] Working on page: 396\n",
            "[*] Working on page: 397\n",
            "[*] Working on page: 398\n",
            "[*] Working on page: 399\n",
            "[*] Working on page: 400\n",
            "[*] Working on page: 401\n",
            "[*] Working on page: 402\n",
            "[*] Working on page: 403\n",
            "[*] Working on page: 404\n",
            "[*] Working on page: 405\n",
            "[*] Working on page: 406\n",
            "[*] Working on page: 407\n",
            "[*] Working on page: 408\n",
            "[*] Working on page: 409\n",
            "[*] Working on page: 410\n",
            "[*] Working on page: 411\n",
            "[*] Working on page: 412\n",
            "[*] Working on page: 413\n",
            "[*] Working on page: 414\n",
            "[*] Working on page: 415\n",
            "[*] Working on page: 416\n",
            "[*] Working on page: 417\n",
            "[*] Working on page: 418\n",
            "[*] Working on page: 419\n",
            "[*] Working on page: 420\n",
            "[*] Working on page: 421\n",
            "[*] Working on page: 422\n",
            "[*] Working on page: 423\n",
            "[*] Working on page: 424\n",
            "[*] Working on page: 425\n",
            "[*] Working on page: 426\n",
            "[*] Working on page: 427\n",
            "[*] Working on page: 428\n",
            "[*] Working on page: 429\n",
            "[*] Working on page: 430\n",
            "[*] Working on page: 431\n",
            "[*] Working on page: 432\n",
            "[*] Working on page: 433\n",
            "[*] Working on page: 434\n",
            "[*] Working on page: 435\n",
            "[*] Working on page: 436\n",
            "[*] Working on page: 437\n",
            "[*] Working on page: 438\n",
            "[*] Working on page: 439\n",
            "[*] Working on page: 440\n",
            "[*] Working on page: 441\n",
            "[*] Working on page: 442\n",
            "[*] Working on page: 443\n",
            "[*] Working on page: 444\n",
            "[*] Working on page: 445\n",
            "[*] Working on page: 446\n",
            "[*] Working on page: 447\n",
            "[*] Working on page: 448\n",
            "[*] Working on page: 449\n",
            "[*] Working on page: 450\n",
            "[*] Working on page: 451\n",
            "[*] Working on page: 452\n",
            "[*] Working on page: 453\n",
            "[*] Working on page: 454\n",
            "[*] Working on page: 455\n",
            "[*] Working on page: 456\n",
            "[*] Working on page: 457\n",
            "[*] Working on page: 458\n",
            "[*] Working on page: 459\n",
            "[*] Working on page: 460\n",
            "[*] Working on page: 461\n",
            "[*] Working on page: 462\n",
            "[*] Working on page: 463\n",
            "[*] Working on page: 464\n",
            "[*] Working on page: 465\n",
            "[*] Working on page: 466\n",
            "[*] Working on page: 467\n",
            "[*] Working on page: 468\n",
            "[*] Working on page: 469\n",
            "[*] Working on page: 470\n",
            "[*] Working on page: 471\n",
            "[*] Working on page: 472\n",
            "[*] Working on page: 473\n",
            "[*] Working on page: 474\n",
            "[*] Working on page: 475\n",
            "[*] Working on page: 476\n",
            "[*] Working on page: 477\n",
            "[*] Working on page: 478\n",
            "[*] Working on page: 479\n",
            "[*] Working on page: 480\n",
            "[*] Working on page: 481\n",
            "[*] Working on page: 482\n",
            "[*] Working on page: 483\n",
            "[*] Working on page: 484\n",
            "[*] Working on page: 485\n",
            "[*] Working on page: 486\n",
            "[*] Working on page: 487\n",
            "[*] Working on page: 488\n",
            "[*] Working on page: 489\n",
            "[*] Working on page: 490\n",
            "[*] Working on page: 491\n",
            "[*] Working on page: 492\n",
            "[*] Working on page: 493\n",
            "[*] Working on page: 494\n",
            "[*] Working on page: 495\n",
            "[*] Working on page: 496\n",
            "[*] Working on page: 497\n",
            "[*] Working on page: 498\n",
            "[*] Working on page: 499\n",
            "[*] Working on page: 500\n",
            "[*] Working on page: 501\n",
            "[*] Working on page: 502\n",
            "[*] Working on page: 503\n",
            "[*] Working on page: 504\n",
            "[*] Working on page: 505\n",
            "[*] Working on page: 506\n",
            "[*] Working on page: 507\n",
            "[*] Working on page: 508\n",
            "[*] Working on page: 509\n",
            "[*] Working on page: 510\n",
            "[*] Working on page: 511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDZ7i6_EGZz1"
      },
      "source": [
        "Read in csv and save to dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhZPY6iUZkys"
      },
      "source": [
        "df1 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/TLOU2_reviews.csv\",engine='python' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzSSBmGUGtlF"
      },
      "source": [
        "Drop any null values, as these generate a huge problem with embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smtZqdMncg6z"
      },
      "source": [
        "df1 = df1.dropna()\n",
        "del df1['Unnamed: 0']\n",
        "df1 = df1.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_84T1Aq6vp5"
      },
      "source": [
        "Read to csv to convert the reviews to English in Google Sheets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKp1hNPJd49p"
      },
      "source": [
        "df1.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/TLOU2_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuaF3cLIOdGV"
      },
      "source": [
        "# 2) Pokemon Sword Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwSM78z-SWtM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "fb2864ca-ae03-473d-9d91-e0babbca60ff"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/switch/pokemon-sword/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find('span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "      \n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "\n",
        "    df2 = pd.DataFrame(review_dict)\n",
        "    df2 = df2.drop(columns=['name', 'date'])\n",
        "    df2.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/pokemon_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__': \n",
        "    \n",
        "    chunkIt(range(27), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSf4OTHqK6W4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "22403603-f510-4384-8e9e-e3f520118701"
      },
      "source": [
        "df2 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/pokemon_reviews.csv\",engine='python' )\n",
        "df2 = df2.dropna()\n",
        "del df2['Unnamed: 0']\n",
        "df2 = df2.reset_index(drop=True)\n",
        "print(df2)\n",
        "print(df2['rating'].value_counts())\n",
        "df2.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/pokemon_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        5.0  My copy of Sword and Shield came in the mail a...\n",
            "1        3.0  I really wish the games were good. I don't lea...\n",
            "2        5.0  The game is meh not even counting the Pokemon ...\n",
            "3        3.0  This is what happen if a company realize that ...\n",
            "4        4.0  Let us address the elephant in the room first....\n",
            "...      ...                                                ...\n",
            "2670     2.0  The game is indeed controversial. There are a ...\n",
            "2671     5.0  I had a lot of fun with Pokemon Sword and Shie...\n",
            "2672     5.0  Decepcionante. Esperaba poco del juego después...\n",
            "2673     5.0  This game is so disappointing and it feels ver...\n",
            "2674     2.0  Lazy and rushed game for $60\\r - National Dex ...\n",
            "\n",
            "[2675 rows x 2 columns]\n",
            "0.0     643\n",
            "10.0    403\n",
            "1.0     262\n",
            "4.0     225\n",
            "3.0     218\n",
            "2.0     214\n",
            "9.0     201\n",
            "8.0     175\n",
            "5.0     125\n",
            "6.0     106\n",
            "7.0     103\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQeAVtWCYqq_"
      },
      "source": [
        "# 3) Modern Warfare Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEXnHF5FL8SY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "010c4e6d-d63c-4fa7-f98c-8f9f52c2a9b7"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/call-of-duty-modern-warfare/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df3 = pd.DataFrame(review_dict)\n",
        "    df3 = df3.drop(columns=['name', 'date'])\n",
        "    df3.to_csv(r\"drive/My Drive/csv's/MW_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(42), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxxnolJmM3iW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "326ce3e7-26d8-4b90-c513-e98548c488cc"
      },
      "source": [
        "df3 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/MW_reviews.csv\",engine='python' )\n",
        "df3 = df3.dropna()\n",
        "del df3['Unnamed: 0']\n",
        "df3 = df3.reset_index(drop=True)\n",
        "print(df3)\n",
        "print(df3['rating'].value_counts())\n",
        "df3.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/MW_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  Playtime 22 days. Its my first cod. Love it. G...\n",
            "1        0.0  The developers just decided to accuse the Russ...\n",
            "2        0.0  This game is nothing but a russophobic propaga...\n",
            "3        0.0  Creators of this game will end like Joseph Goe...\n",
            "4        0.0  I even created a Metacritic account for the re...\n",
            "...      ...                                                ...\n",
            "3985     7.0  The only complaint I have of the game is the s...\n",
            "3986     0.0  Esse jogo é  maior decepção que eu já tive.  A...\n",
            "3987    10.0  A pretty good and super fun game. The sounds a...\n",
            "3988     0.0  This game HAD so much potential. The mechanics...\n",
            "3989     5.0  Campaign: 8/10, easily the best cod campaign, ...\n",
            "\n",
            "[3990 rows x 2 columns]\n",
            "0.0     2037\n",
            "10.0     818\n",
            "1.0      369\n",
            "9.0      188\n",
            "8.0      146\n",
            "2.0      120\n",
            "3.0       72\n",
            "7.0       71\n",
            "4.0       71\n",
            "6.0       50\n",
            "5.0       48\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArEFuxP_tDPB"
      },
      "source": [
        "# 4) Death Stranding Reviews:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LfWwmvQtHDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a28c38d5-30bb-48fe-dfc8-b45635484163"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/death-stranding/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df4 = pd.DataFrame(review_dict)\n",
        "    df4 = df4.drop(columns=['name', 'date'])\n",
        "    df4.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/DS_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(76), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n",
            "[*] Working on page: 42\n",
            "[*] Working on page: 43\n",
            "[*] Working on page: 44\n",
            "[*] Working on page: 45\n",
            "[*] Working on page: 46\n",
            "[*] Working on page: 47\n",
            "[*] Working on page: 48\n",
            "[*] Working on page: 49\n",
            "[*] Working on page: 50\n",
            "[*] Working on page: 51\n",
            "[*] Working on page: 52\n",
            "[*] Working on page: 53\n",
            "[*] Working on page: 54\n",
            "[*] Working on page: 55\n",
            "[*] Working on page: 56\n",
            "[*] Working on page: 57\n",
            "[*] Working on page: 58\n",
            "[*] Working on page: 59\n",
            "[*] Working on page: 60\n",
            "[*] Working on page: 61\n",
            "[*] Working on page: 62\n",
            "[*] Working on page: 63\n",
            "[*] Working on page: 64\n",
            "[*] Working on page: 65\n",
            "[*] Working on page: 66\n",
            "[*] Working on page: 67\n",
            "[*] Working on page: 68\n",
            "[*] Working on page: 69\n",
            "[*] Working on page: 70\n",
            "[*] Working on page: 71\n",
            "[*] Working on page: 72\n",
            "[*] Working on page: 73\n",
            "[*] Working on page: 74\n",
            "[*] Working on page: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEU_ycRqtkis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "646d5cff-8de0-48e5-e94c-8af72702db84"
      },
      "source": [
        "df4 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/DS_reviews.csv\",engine='python' )\n",
        "df4 = df4.dropna()\n",
        "del df4['Unnamed: 0']\n",
        "df4 = df4.reset_index(drop=True)\n",
        "print(df4)\n",
        "print(df4['rating'].value_counts())\n",
        "df4.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/DS_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        2.0  Definitely a waste of $.  Good graphics and od...\n",
            "1        4.0  Is a very monotonous game,  it has great music...\n",
            "2        2.0  The trailers for this game sold me the idea of...\n",
            "3        0.0  If you ever believed there was a god, then you...\n",
            "4        4.0                 The most boring game in the world \n",
            "...      ...                                                ...\n",
            "7182     9.0  Primero que nada, visualmente el juego es extr...\n",
            "7183     0.0  I don't know starting from when, the celebrity...\n",
            "7184     9.0  Amazing game that clearly you love it or you h...\n",
            "7185     9.0  Every once in a while, there is an specific pi...\n",
            "7186    10.0  Hekayə 10Atmosfer 10Grafika 10Musiqi 10Hideo K...\n",
            "\n",
            "[7187 rows x 2 columns]\n",
            "10.0    3405\n",
            "0.0     1015\n",
            "9.0      951\n",
            "8.0      421\n",
            "1.0      370\n",
            "2.0      225\n",
            "3.0      217\n",
            "4.0      184\n",
            "5.0      162\n",
            "7.0      129\n",
            "6.0      108\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQYaxqZmyMF0"
      },
      "source": [
        "# 5) Overwatch Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKX15COE1Ens",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "23edbe95-67fc-4f67-b1c9-a263ec37322c"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/pc/overwatch/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df5 = pd.DataFrame(review_dict)\n",
        "    df5 = df5.drop(columns=['name', 'date'])\n",
        "    df5.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/overwatch_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(13), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRmpCOS-1Y_b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "fd6462a1-f9bf-4d24-9c26-61657ecd4b9e"
      },
      "source": [
        "df5 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/overwatch_reviews.csv\",engine='python' )\n",
        "df5 = df5.dropna()\n",
        "del df5['Unnamed: 0']\n",
        "df5 = df5.reset_index(drop=True)\n",
        "print(df5)\n",
        "print(df5['rating'].value_counts())\n",
        "df5.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/overwatch_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        1.0  Now that I have played this game for over ONE ...\n",
            "1        0.0  Choose a hero from a huge cast of characters!*...\n",
            "2        0.0  Game is worth a 6 at best. Limited maps. POTG ...\n",
            "3        0.0  The game is marketed as a game possessing pers...\n",
            "4        0.0  What happened to Blizzard? It's all style over...\n",
            "...      ...                                                ...\n",
            "1286     4.0  The game is good, the idea is interesting, you...\n",
            "1287    10.0  This game is really epic. I Really recommend t...\n",
            "1288     0.0  Игра скатилась.В каждой 2-3 катке есть ливер/т...\n",
            "1289     0.0  I always looked forward to playing Overwatch b...\n",
            "1290     0.0  Valorant and Overwatch are in a constant gridl...\n",
            "\n",
            "[1291 rows x 2 columns]\n",
            "10.0    300\n",
            "0.0     274\n",
            "9.0     157\n",
            "8.0      91\n",
            "5.0      84\n",
            "6.0      82\n",
            "7.0      73\n",
            "1.0      68\n",
            "4.0      63\n",
            "2.0      51\n",
            "3.0      48\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE7WE0Jymi09"
      },
      "source": [
        "# 6) Red Dead Redemption 2 Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja-U1UHimnde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "caa36c79-1235-471d-86de-1cc9911cffcc"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/red-dead-redemption-2/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df6 = pd.DataFrame(review_dict)\n",
        "    df6 = df6.drop(columns=['name', 'date'])\n",
        "    df6.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/RDR2_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(36), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOfMrsFnX9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "90a211e4-302e-4f65-fd1f-9588eff293c2"
      },
      "source": [
        "df6 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/RDR2_reviews.csv\",engine='python' )\n",
        "df6 = df6.dropna()\n",
        "del df6['Unnamed: 0']\n",
        "df6 = df6.reset_index(drop=True)\n",
        "print(df6)\n",
        "print(df6['rating'].value_counts())\n",
        "df6.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/RDR2_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        7.0  I'll start off by saying it's impossible to ta...\n",
            "1        7.0  All the good things you've heard about this ga...\n",
            "2        7.0  Beautiful graphics, excellent voice acting, lo...\n",
            "3        7.0  Fair review of RDR2\\r I'm almost 15% finished ...\n",
            "4        8.0  The 10 score reviews here must be some hardcor...\n",
            "...      ...                                                ...\n",
            "3333     9.0  Great story ,  and amazing game .. though at t...\n",
            "3334    10.0  Um dos melhores jogos que já joguei na minha v...\n",
            "3335     0.0  скучнее только кодзимовский высер.мультиплеер ...\n",
            "3336    10.0  I spent many hours of my life hunting, fishing...\n",
            "3337    10.0  Easily one of the greatest gaming experiences ...\n",
            "\n",
            "[3338 rows x 2 columns]\n",
            "10.0    1821\n",
            "9.0      449\n",
            "8.0      231\n",
            "0.0      177\n",
            "7.0      166\n",
            "5.0      121\n",
            "6.0      114\n",
            "4.0       91\n",
            "1.0       63\n",
            "3.0       59\n",
            "2.0       46\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djadVOZYqAIK"
      },
      "source": [
        "# 7) Minecraft Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_kqza2WqGZm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1dc20879-8e37-40fb-a17d-b65430a0397b"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/pc/minecraft/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df7 = pd.DataFrame(review_dict)\n",
        "    df7 = df7.drop(columns=['name', 'date'])\n",
        "    df7.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/minecraft_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(15), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34S3biSHsZ-C"
      },
      "source": [
        "df7 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/minecraft_reviews.csv\",engine='python' )\n",
        "df7 = df7.dropna()\n",
        "del df7['Unnamed: 0']\n",
        "df7 = df7.reset_index(drop=True)\n",
        "print(df7)\n",
        "print(df7['rating'].value_counts())\n",
        "df7.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/minecraft_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ybBqQ9gt188"
      },
      "source": [
        "# 8) Battlefront (2015) Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl25K-JJt-Ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e783691e-9a12-4e29-c6d0-a89b5bac235d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/star-wars-battlefront/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df8 = pd.DataFrame(review_dict)\n",
        "    df8 = df8.drop(columns=['name', 'date'])\n",
        "    df8.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/battlefront_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(8), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siqWzBgTuR2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "4f9c7ab0-9c21-4eec-d17b-c0cf3493dea9"
      },
      "source": [
        "df8 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/battlefront_reviews.csv\",engine='python' )\n",
        "df8 = df8.dropna()\n",
        "del df8['Unnamed: 0']\n",
        "df8 = df8.reset_index(drop=True)\n",
        "print(df8)\n",
        "print(df8['rating'].value_counts())\n",
        "df8.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/battlefront_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       5.0  Looks amazing! Seriously, I mean WOW! You do f...\n",
            "1       4.0  Good visuals. Gets boring quickly once the nov...\n",
            "2       3.0  The graphics in this game are absolutely amazi...\n",
            "3       4.0  While it's true Star Wars fans have been clamo...\n",
            "4       4.0  Game sounds and looks incredible. thats where ...\n",
            "..      ...                                                ...\n",
            "745     6.0  No microtransactions :-). But nothing like the...\n",
            "746     2.0  Star Wars Battlefront is one of the most beaut...\n",
            "747     8.0  Отличная игра. Графика для 2015 года просто на...\n",
            "748     0.0  This game does not even have a bloody single-p...\n",
            "749     5.0  5/10 - No sé si habrá sido que no entendí la f...\n",
            "\n",
            "[750 rows x 2 columns]\n",
            "0.0     118\n",
            "5.0      85\n",
            "8.0      77\n",
            "4.0      75\n",
            "7.0      71\n",
            "6.0      63\n",
            "9.0      58\n",
            "10.0     54\n",
            "2.0      52\n",
            "3.0      50\n",
            "1.0      47\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCGjCDaKhvwn"
      },
      "source": [
        "# 9) Black Ops 3 Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVU49agAhz0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "5f4e51bb-12ff-457b-f529-07e98ec7d0ab"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/call-of-duty-black-ops-iii/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df9 = pd.DataFrame(review_dict)\n",
        "    df9 = df9.drop(columns=['name', 'date'])\n",
        "    df9.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/BO3_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(7), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGQteiv6ieY0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "5edd2497-4a04-4243-8af8-98d5fce43945"
      },
      "source": [
        "df9 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/BO3_reviews.csv\",engine='python' )\n",
        "df9 = df9.dropna()\n",
        "del df9['Unnamed: 0']\n",
        "df9 = df9.reset_index(drop=True)\n",
        "print(df9)\n",
        "print(df9['rating'].value_counts())\n",
        "df9.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/BO3_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0         7  Giving this game a zero is silly, BUT giving a...\n",
            "1         4  I can barley withstand Campaign. Same tricks d...\n",
            "2         3  Lets look at Halo 5, not the best game in the ...\n",
            "3         4  Absolutely no innovation and honestly a massiv...\n",
            "4         8  Black Ops 3 isn't a masterpiece but I believe ...\n",
            "..      ...                                                ...\n",
            "611       6  Black ops 3 is a mixed bag for me. On one hand...\n",
            "612       8  despite being futuristic has no denying that t...\n",
            "613       6  The solo campaign is absolutely trash, but thi...\n",
            "614      10  Favorite game of all time. I love everything a...\n",
            "615       6  okay let me get this over with the campain and...\n",
            "\n",
            "[616 rows x 2 columns]\n",
            "0     92\n",
            "8     82\n",
            "10    80\n",
            "7     62\n",
            "9     57\n",
            "5     48\n",
            "6     44\n",
            "1     44\n",
            "4     43\n",
            "3     37\n",
            "2     27\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbgXefmfjKQT"
      },
      "source": [
        "# 10) Assassins Creed Odyssey Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mD9dNUfjPWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "717d1763-193f-4239-963b-0fd18974785e"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/assassins-creed-odyssey/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df10 = pd.DataFrame(review_dict)\n",
        "    df10 = df10.drop(columns=['name', 'date'])\n",
        "    df10.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/ASO_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(7), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOF3AokRjjdF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "f26115f0-87cb-46e4-cead-88d972bac823"
      },
      "source": [
        "df10 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/ASO_reviews.csv\",engine='python' )\n",
        "df10 = df10.dropna()\n",
        "del df10['Unnamed: 0']\n",
        "df10 = df10.reset_index(drop=True)\n",
        "print(df10)\n",
        "print(df10['rating'].value_counts())\n",
        "df10.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/ASO_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  (I haven't played origins) This Assassin's Cre...\n",
            "1      10.0  Don't listen to all the ignorant people compla...\n",
            "2      10.0  I cannot understand the bad reviews for this g...\n",
            "3      10.0  It doesnt deserve such low score. I played all...\n",
            "4       5.0  Nah, not a fan of the new formula. Side activi...\n",
            "..      ...                                                ...\n",
            "693     9.0  Durée de vie incroyable, graphisme superbe, bo...\n",
            "694    10.0  I’ve been waiting for AC franchise to come up ...\n",
            "695     8.0  Assassin's Creed: Odyssey is the best Assassin...\n",
            "696     8.0  I think this game was the second game of Assas...\n",
            "697     2.0  It's Origins but more tedious.This is kind of ...\n",
            "\n",
            "[698 rows x 2 columns]\n",
            "10.0    184\n",
            "0.0     105\n",
            "9.0      92\n",
            "8.0      77\n",
            "7.0      51\n",
            "6.0      39\n",
            "5.0      39\n",
            "1.0      34\n",
            "4.0      34\n",
            "3.0      25\n",
            "2.0      18\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6evlpiLSkFSd"
      },
      "source": [
        "# 11) No Man's Sky Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un2g7lPqkIS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49cb35cd-7981-4500-cba9-c74d496289fc"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/no-mans-sky/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df11 = pd.DataFrame(review_dict)\n",
        "    df11 = df11.drop(columns=['name', 'date'])\n",
        "    df11.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/NMS_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(14), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-5659:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 159, in _new_conn\n",
            "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\", line 80, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\", line 70, in create_connection\n",
            "    sock.connect(sa)\n",
            "TimeoutError: [Errno 110] Connection timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
            "    chunked=chunked)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 343, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 839, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 301, in connect\n",
            "    conn = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\", line 168, in _new_conn\n",
            "    self, \"Failed to establish a new connection: %s\" % e)\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f00066a3f60>: Failed to establish a new connection: [Errno 110] Connection timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
            "    _stacktrace=sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\", line 399, in increment\n",
            "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
            "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.metacritic.com', port=443): Max retries exceeded with url: /game/playstation-4/no-mans-sky/user-reviews?page=2 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f00066a3f60>: Failed to establish a new connection: [Errno 110] Connection timed out',))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-69-fbc741d1732a>\", line 5, in scrap_page\n",
            "    response = requests.get(url, headers=user_agent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 76, in get\n",
            "    return request('get', url, params=params, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/api.py\", line 61, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/adapters.py\", line 516, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.metacritic.com', port=443): Max retries exceeded with url: /game/playstation-4/no-mans-sky/user-reviews?page=2 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f00066a3f60>: Failed to establish a new connection: [Errno 110] Connection timed out',))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY5_okfJki7u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "4177d8ac-12e3-4706-d718-50947139b92a"
      },
      "source": [
        "df11 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/NMS_reviews.csv\",engine='python' )\n",
        "df11 = df11.dropna()\n",
        "del df11['Unnamed: 0']\n",
        "df11 = df11.reset_index(drop=True)\n",
        "print(df11)\n",
        "print(df11['rating'].value_counts())\n",
        "df11.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/NMS_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        3.0  No Man's Sky really a different type of game. ...\n",
            "1        3.0  This game is absolutely massive, and has a dec...\n",
            "2        7.0  My original review is 2 years old and based of...\n",
            "3        5.0  NMS can be described as a surprisingly SMALL g...\n",
            "4        3.0  What appeared to be beautiful at the initial r...\n",
            "...      ...                                                ...\n",
            "1186     6.0  It got better, but it is still boring. It's an...\n",
            "1187     9.0  Es un hermoso y más si amas la astronomía tan ...\n",
            "1188     0.0  Engañar a la gente con falsas promesas y carac...\n",
            "1189    10.0  I've been playing this game since day one. At ...\n",
            "1190     2.0  Has a nice idea, but poorly executed, just fel...\n",
            "\n",
            "[1191 rows x 2 columns]\n",
            "10.0    300\n",
            "0.0     165\n",
            "9.0     132\n",
            "8.0     120\n",
            "3.0      78\n",
            "5.0      76\n",
            "1.0      73\n",
            "4.0      72\n",
            "6.0      60\n",
            "7.0      60\n",
            "2.0      55\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHfPtChsrlh6"
      },
      "source": [
        "# 12) Halo 5 Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y8eoAbdrnZY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "9c29b252-34a4-4ae4-cad5-3d779af55397"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/xbox-one/halo-5-guardians/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df12 = pd.DataFrame(review_dict)\n",
        "    df12 = df12.drop(columns=['name', 'date'])\n",
        "    df12.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/HALO5_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(10), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVgjz8ShsBen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "a6bbf4c5-26ac-4a81-e7e3-e7caf9865cc1"
      },
      "source": [
        "df12 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/HALO5_reviews.csv\",engine='python' )\n",
        "df12 = df12.dropna()\n",
        "del df12['Unnamed: 0']\n",
        "df12 = df12.reset_index(drop=True)\n",
        "print(df12)\n",
        "print(df12['rating'].value_counts())\n",
        "df12.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/HALO5_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       6.0  Halo 5 is all in a polarizing experience. One ...\n",
            "1       6.0  TL;DR Campaign 4/10 | Mutiplayer 7/10 - - Over...\n",
            "2       6.0  The worst Halo for me and cannot understand an...\n",
            "3       7.0  General game play:\\r-Movement feels decently f...\n",
            "4       8.0  I cannot explain how horrendous this game is I...\n",
            "..      ...                                                ...\n",
            "945     5.0  Only a few acts as Masterchief, new characters...\n",
            "946    10.0  It's One of my favorite game, I hope to play t...\n",
            "947     0.0  Купил Xbox ради этой игры а потом распидарасил...\n",
            "948     7.0  Si no fuera por la campaña publicitaria algo \"...\n",
            "949     6.0  Siempre pensé que la revolución en Halo llegar...\n",
            "\n",
            "[950 rows x 2 columns]\n",
            "10.0    252\n",
            "9.0     125\n",
            "8.0     120\n",
            "0.0     107\n",
            "7.0      78\n",
            "6.0      72\n",
            "4.0      58\n",
            "5.0      52\n",
            "3.0      31\n",
            "1.0      31\n",
            "2.0      24\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBk8Dx5gs_YF"
      },
      "source": [
        "# 13) Borderlands 3 Metacritic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljEyyWVMtBmK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "eb3369b4-ae32-42cf-a01d-f685fb755f68"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/pc/borderlands-3/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df13 = pd.DataFrame(review_dict)\n",
        "    df13 = df13.drop(columns=['name', 'date'])\n",
        "    df13.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/BL3_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(12), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXhbsH29tZUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "43662dfa-edea-4a1f-af63-494ecea6beb8"
      },
      "source": [
        "df13 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/BL3_reviews.csv\",engine='python' )\n",
        "df13 = df13.dropna()\n",
        "del df13['Unnamed: 0']\n",
        "df13 = df13.reset_index(drop=True)\n",
        "print(df13)\n",
        "print(df13['rating'].value_counts())\n",
        "df13.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/BL3_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        0.0  I personally did not experience any major issu...\n",
            "1        0.0  What's the point of collecting eridium if it's...\n",
            "2        1.0  Game is unacceptably buggy as of now. My PC is...\n",
            "3        0.0  This game has everything to be considered a go...\n",
            "4        2.0  1080 Ti and cant run normal frames. It micro s...\n",
            "...      ...                                                ...\n",
            "1152     4.0  Este jugo es bueno, le pongo esta nota por las...\n",
            "1153     6.0  Gameplay is great and improved from the pre-qu...\n",
            "1154     7.0  Es muy buen juego pero cambia tan poco que me ...\n",
            "1155     0.0  I love Borderlands, I love 2k, Borderlands 3.....\n",
            "1156     7.0  This game could've had it all: the gameplay, t...\n",
            "\n",
            "[1157 rows x 2 columns]\n",
            "10.0    279\n",
            "0.0     245\n",
            "9.0     150\n",
            "8.0     100\n",
            "4.0      74\n",
            "1.0      72\n",
            "2.0      56\n",
            "5.0      53\n",
            "7.0      50\n",
            "3.0      43\n",
            "6.0      35\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK0s3B84t_TB"
      },
      "source": [
        "# 14) Assassins Creed Origins Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D_nJ7onuC-V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "4c3ab993-a291-4e43-ffe3-526a7d28a7ab"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/assassins-creed-origins/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df14 = pd.DataFrame(review_dict)\n",
        "    df14 = df14.drop(columns=['name', 'date'])\n",
        "    df14.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/ACOR_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(7), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U8d1ZRjulYQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "8ce2fc80-5bac-4cd0-c7c9-273eb81e152e"
      },
      "source": [
        "df14 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/ACOR_reviews.csv\",engine='python' )\n",
        "df14 = df14.dropna()\n",
        "del df14['Unnamed: 0']\n",
        "df14 = df14.reset_index(drop=True)\n",
        "print(df14)\n",
        "print(df14['rating'].value_counts())\n",
        "df14.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/ACOR_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       9.0  The best Assassin's Creed game by a mile, and ...\n",
            "1      10.0  Best Assassins creed game since Black Flag! Lo...\n",
            "2      10.0  Played for a couple hours I have no issues at ...\n",
            "3       4.0  HONEST REVIEW: 4/10 mediocre. Honestly I wasn'...\n",
            "4      10.0  AC origins took a year off of development and ...\n",
            "..      ...                                                ...\n",
            "574     7.0  First off, this game looks amazing. The world ...\n",
            "575     9.0  Tiene una buena trama, es innovador dentro de ...\n",
            "576    10.0  best assassins creed we had in a while since t...\n",
            "577    10.0  Ancient Egypt simulator. Absolutely loved it.\\...\n",
            "578     7.0  Good gameplay and visuals, however for an Assa...\n",
            "\n",
            "[579 rows x 2 columns]\n",
            "10.0    151\n",
            "9.0     104\n",
            "8.0      94\n",
            "0.0      62\n",
            "7.0      57\n",
            "4.0      27\n",
            "5.0      26\n",
            "6.0      21\n",
            "1.0      18\n",
            "2.0      10\n",
            "3.0       9\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMLI2Rc5vIFa"
      },
      "source": [
        "# 15) Order1886 Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSuk5qcGvHh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5b2dafdd-9ac5-4f10-ed5e-08b96a743d3c"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-order-1886/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df15 = pd.DataFrame(review_dict)\n",
        "    df15 = df15.drop(columns=['name', 'date'])\n",
        "    df15.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/1886_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(11), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkRK71rpvo7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d8e711b6-55e6-41ff-deec-69418cd097ec"
      },
      "source": [
        "df15 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/1886_reviews.csv\",engine='python' )\n",
        "df15 = df15.dropna()\n",
        "del df15['Unnamed: 0']\n",
        "df15 = df15.reset_index(drop=True)\n",
        "print(df15)\n",
        "print(df15['rating'].value_counts())\n",
        "df15.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/1886_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       6.0  I will start off by saying unfortunately a lot...\n",
            "1       6.0  i have finished the game on hard difficulty an...\n",
            "2       6.0  The Order 1886 struggles with finding identity...\n",
            "3       7.0  This game will put you in a big dilemma. It is...\n",
            "4       4.0  There is several things to like about the game...\n",
            "..      ...                                                ...\n",
            "980     8.0  Great single player game. If you play it for w...\n",
            "981     4.0  HahahahahahahahahahagahahahahagahahahahahahAha...\n",
            "982     6.0  Order 1886 was probably intended rather a tech...\n",
            "983     9.0  Хорошая игра на несколько вечеров. Интересный ...\n",
            "984    10.0  the first game i played on my Ps4 that i playe...\n",
            "\n",
            "[985 rows x 2 columns]\n",
            "10.0    233\n",
            "8.0     180\n",
            "9.0     147\n",
            "7.0      98\n",
            "6.0      89\n",
            "5.0      63\n",
            "0.0      49\n",
            "4.0      45\n",
            "2.0      33\n",
            "1.0      24\n",
            "3.0      24\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDzZBjkVwI-S"
      },
      "source": [
        "# 16) Days Gone Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzEIEs_pwLEK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "c4b698bb-77ec-459e-d991-2fb1f0b07e5f"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/days-gone/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df16 = pd.DataFrame(review_dict)\n",
        "    # df16 = df15.drop(columns=['name', 'date'])\n",
        "    df16.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/DG_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(26), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl9bKVcfxD_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "4405cb1b-614c-4f82-893f-f820fd94383e"
      },
      "source": [
        "df16 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/DG_reviews.csv\",engine='python' )\n",
        "df16 = df16.dropna()\n",
        "del df16['Unnamed: 0']\n",
        "df16 = df16.reset_index(drop=True)\n",
        "print(df16)\n",
        "print(df16['rating'].value_counts())\n",
        "df16.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/DG_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                name  ...                                             review\n",
            "0         umairbasit  ...  The reviews are so disappointing for this amaz...\n",
            "1              Colle  ...  I am now 30 hours into the game and I have to ...\n",
            "2     theydisintegr8  ...  8.5 game for me. Lots of fun and loving the fe...\n",
            "3          the103don  ...  Having played games since the early 80s I have...\n",
            "4           Fenrir79  ...  Awesome game. This game feels like a great Joh...\n",
            "...              ...  ...                                                ...\n",
            "2459   DennisWaltwin  ...  Jogo com uma bela história e uma trilha sonora...\n",
            "2460       EderGamer  ...  Muita ação e aventura ao mesmo tempo, sem cont...\n",
            "2461         GabFire  ...  Apesar do jogo apresentar um monte de problema...\n",
            "2462          Grippa  ...  hhohohohoohohohhohohohohoohohohohohohohohohoho...\n",
            "2463       alexslork  ...  DAYS GONE – a breath of fresh air in its genre...\n",
            "\n",
            "[2464 rows x 4 columns]\n",
            "10.0    1057\n",
            "9.0      661\n",
            "8.0      360\n",
            "0.0      103\n",
            "7.0       90\n",
            "5.0       42\n",
            "4.0       40\n",
            "6.0       33\n",
            "1.0       32\n",
            "3.0       26\n",
            "2.0       20\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7axVVurxoTM"
      },
      "source": [
        "# 17) GTA V Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI3AF84Vxqz2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "498ace01-d789-421a-bcc6-4e7496424b6f"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/grand-theft-auto-v/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df17 = pd.DataFrame(review_dict)\n",
        "    df17 = df17.drop(columns=['name', 'date'])\n",
        "    df17.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/GTAV_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(9), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSTeV1dUyF3X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "50b1090c-feeb-47b0-b71e-ff37097e56c9"
      },
      "source": [
        "df17 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/GTAV_reviews.csv\",engine='python' )\n",
        "df17 = df17.dropna()\n",
        "del df17['Unnamed: 0']\n",
        "df17 = df17.reset_index(drop=True)\n",
        "print(df17)\n",
        "print(df17['rating'].value_counts())\n",
        "df17.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/GTAV_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       8.0  GTA V was arguably one of the most anticipated...\n",
            "1      10.0  Small things are important. GTA V has a lot of...\n",
            "2       7.0  PS4 VERSION:\\r Upon first impressions, I thoug...\n",
            "3      10.0  This is not merely a port , many new things ha...\n",
            "4       2.0  Im sorry but i cant get over how bad the contr...\n",
            "..      ...                                                ...\n",
            "718     0.0  it's the same **** as usual. people are gettin...\n",
            "719    10.0  10 masterpiece single play game is Masterpiece...\n",
            "720    10.0  Que se puede decir de este juego? pues eso.......\n",
            "721     9.0  Aunque la historia no es muy buena el resto de...\n",
            "722    10.0  GTA 5 - шедевр\\rGTA ONLINE - донатерское говно...\n",
            "\n",
            "[723 rows x 2 columns]\n",
            "10.0    286\n",
            "9.0     156\n",
            "8.0      85\n",
            "7.0      52\n",
            "0.0      51\n",
            "6.0      27\n",
            "5.0      23\n",
            "4.0      19\n",
            "1.0      10\n",
            "3.0       7\n",
            "2.0       7\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRP0gmE0ymDk"
      },
      "source": [
        "# 18) Arkham Knight Review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxuJSkpVyoYV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "81f111ee-9a4f-4aec-edc4-8fd5674377e5"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/batman-arkham-knight/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df18 = pd.DataFrame(review_dict)\n",
        "    df18 = df18.drop(columns=['name', 'date'])\n",
        "    df18.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/AK_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(9), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9DssnDKy3O6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b8fe298b-9877-4c14-becc-a1935a2d9d96"
      },
      "source": [
        "df18 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/AK_reviews.csv\",engine='python' )\n",
        "df18 = df18.dropna()\n",
        "del df18['Unnamed: 0']\n",
        "df18 = df18.reset_index(drop=True)\n",
        "print(df18)\n",
        "print(df18['rating'].value_counts())\n",
        "df18.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/AK_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Before I review the game, let me tell you how ...\n",
            "1       9.0  Ok....so before we proceed; for all those Batm...\n",
            "2      10.0  I give this game a 10 for the following reason...\n",
            "3       9.0  Note to everyone who gives this, or most other...\n",
            "4      10.0  Batman Arkham Knight is a game that improves o...\n",
            "..      ...                                                ...\n",
            "745     8.0  Following three fantastic games Arkham Knight,...\n",
            "746     8.0  damn good game especially the story not really...\n",
            "747     8.0  Pros:•Good Story•Good Combat•Batmobile•Good Tw...\n",
            "748     7.0  Big Batman game, but my first Batman game.\\r G...\n",
            "749    10.0  This game is breathtaking, words cannot descri...\n",
            "\n",
            "[750 rows x 2 columns]\n",
            "10.0    234\n",
            "9.0     171\n",
            "8.0     139\n",
            "7.0      68\n",
            "6.0      50\n",
            "5.0      30\n",
            "0.0      17\n",
            "4.0      14\n",
            "1.0      11\n",
            "3.0       9\n",
            "2.0       7\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8SDypRyARXf"
      },
      "source": [
        "# 19) Animal Crossing Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX4HhKJZAZJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "f3f309fa-f1e2-458a-ca20-1430a651be0a"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/switch/animal-crossing-new-horizons/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df19 = pd.DataFrame(review_dict)\n",
        "    df19 = df19.drop(columns=['name', 'date'])\n",
        "    df19.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/ACNH_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(37), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLodMOBMAwGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "6faffaa1-06b9-4987-a051-fb35d7dc55a1"
      },
      "source": [
        "df19 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/ACNH_reviews.csv\",engine='python' )\n",
        "df19 = df19.dropna()\n",
        "del df19['Unnamed: 0']\n",
        "df19 = df19.reset_index(drop=True)\n",
        "print(df19)\n",
        "print(df19['rating'].value_counts())\n",
        "df19.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/ACNH_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0          9  The one island per console is pretty stupid, b...\n",
            "1          4  \"One Dictator, Seven Slaves\" same-console mult...\n",
            "2          4  It’s a fun game, very reminiscent of the old A...\n",
            "3          1  To actually review this game I think you'll ne...\n",
            "4          4  I'm cool with the one island per console limit...\n",
            "...      ...                                                ...\n",
            "3533       0  Thank you for the drama and deception with my ...\n",
            "3534       0  Bought this game for the promising coop experi...\n",
            "3535       0  That finds limitations on the island with resp...\n",
            "3536       8  I think that journalists too laud Nintendo gam...\n",
            "3537       2  My boyfriend was super excited for this game, ...\n",
            "\n",
            "[3538 rows x 2 columns]\n",
            "0     1334\n",
            "10     858\n",
            "9      305\n",
            "1      300\n",
            "2      166\n",
            "8      145\n",
            "3      113\n",
            "4      112\n",
            "5      102\n",
            "6       55\n",
            "7       48\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUO54SYJBhbK"
      },
      "source": [
        "# 20) The Last Guardian review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "posysshlBn_4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "3ce196f4-52d6-4c79-d351-7ccfbe5b15b4"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-last-guardian/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df20 = pd.DataFrame(review_dict)\n",
        "    df20 = df20.drop(columns=['name', 'date'])\n",
        "    df20.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/LG_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(6), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcp75s9OB3gI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d1f4910e-aea3-4099-a485-9fc1c57d6a67"
      },
      "source": [
        "df20 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/LG_reviews.csv\",engine='python' )\n",
        "df20 = df20.dropna()\n",
        "del df20['Unnamed: 0']\n",
        "df20 = df20.reset_index(drop=True)\n",
        "print(df20)\n",
        "print(df20['rating'].value_counts())\n",
        "df20.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/LG_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       8.0  If you accept that this game is more of an int...\n",
            "1       8.0  I think this game is a revelation. There are a...\n",
            "2       9.0  Es un juego que con todos sus defectos (cámara...\n",
            "3      10.0  Imprescindible para cualquier persona... creo ...\n",
            "4      10.0  I love this game.  I know it's not for everyon...\n",
            "..      ...                                                ...\n",
            "460    10.0  Perfect game. One of the most intense games I'...\n",
            "461     9.0  A very special game.\\rSome clunky game mechani...\n",
            "462    10.0  An emotional journey not for everyone. Trico i...\n",
            "463    10.0  Increíble muy tierno y es buen sucesor  de Sha...\n",
            "464    10.0  This is a masterpeice that proves video games ...\n",
            "\n",
            "[465 rows x 2 columns]\n",
            "10.0    177\n",
            "9.0      70\n",
            "8.0      59\n",
            "7.0      31\n",
            "0.0      27\n",
            "6.0      27\n",
            "5.0      24\n",
            "2.0      16\n",
            "3.0      16\n",
            "1.0       9\n",
            "4.0       9\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJCsxNbCcAF"
      },
      "source": [
        "# 21) Resident Evil 3 Reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWSb9ljBClOv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "914da495-da85-4cf3-9b31-ddc9ebb73ffc"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/resident-evil-3/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df21 = pd.DataFrame(review_dict)\n",
        "    df21 = df21.drop(columns=['name', 'date'])\n",
        "    df21.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/RE3_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(11), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STaRStG3Cx8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "a1652ea2-8722-4005-e798-3ed05f1358aa"
      },
      "source": [
        "df21 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/RE3_reviews.csv\",engine='python' )\n",
        "df21 = df21.dropna()\n",
        "del df21['Unnamed: 0']\n",
        "df21 = df21.reset_index(drop=True)\n",
        "print(df21)\n",
        "print(df21['rating'].value_counts())\n",
        "df21.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/RE3_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       6.0  Call me delusional, call me nostalgic.. but at...\n",
            "1       4.0  Proof that better graphics do not make for a b...\n",
            "2       4.0  Let me be honest on this one, I would love to ...\n",
            "3       0.0  This is a 5 hours DLC!\\rThey tried to compensa...\n",
            "4       0.0  I just made an account to express how disappoi...\n",
            "..      ...                                                ...\n",
            "912     9.0  The feelings I have for this game are very all...\n",
            "913     8.0  Never got a chance to play the old version of ...\n",
            "914     6.0  Es juego corto, te toma 4 horas y no es muy re...\n",
            "915     8.0  Esta entrega sobre todo la saga entera en mi o...\n",
            "916     5.0  Es un juego con una dinámica de control y unos...\n",
            "\n",
            "[917 rows x 2 columns]\n",
            "10.0    182\n",
            "0.0     120\n",
            "8.0     110\n",
            "9.0     109\n",
            "7.0      77\n",
            "6.0      71\n",
            "5.0      65\n",
            "4.0      65\n",
            "1.0      50\n",
            "3.0      36\n",
            "2.0      32\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh-tb49vDs4Q"
      },
      "source": [
        "# 22) Gears of War 4 Review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwxFtKgIDwdy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "7447d3d6-e972-4a0c-f745-6c74f6256c3d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/xbox-one/gears-of-war-4/user-reviews?page=' + str(\n",
        "        page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "            # scrap_page(page, review_dict)\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df22 = pd.DataFrame(review_dict)\n",
        "    df22 = df22.drop(columns=['name', 'date'])\n",
        "    df22.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/GOW4_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(0,5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HxIq5NOEGON",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9cabaef0-ad6a-43b9-f474-06052f6bc386"
      },
      "source": [
        "df22 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/GOW4_reviews.csv\",engine='python' )\n",
        "df22 = df22.dropna()\n",
        "del df22['Unnamed: 0']\n",
        "df22 = df22.reset_index(drop=True)\n",
        "print(df22)\n",
        "print(df22['rating'].value_counts())\n",
        "df22.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/GOW4_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       9.0  Ok so. Firstly i want to address the people gi...\n",
            "1       9.0  Really not sure what game some of these people...\n",
            "2      10.0  Are people actually giving this game a rating ...\n",
            "3       7.0  As expected, the 0 ratings ALL come from peopl...\n",
            "4       0.0  This was absolute garbage. I refunded this the...\n",
            "..      ...                                                ...\n",
            "370    10.0  Excelente juego, un bueb giro, grafica e histo...\n",
            "371     9.0  Buen juego sin embargo no llena un pequeño vac...\n",
            "372     8.0  Much like what Halo 4 was to the original tril...\n",
            "373     9.0  O Jogo tem uma ótima história! Com gráficos im...\n",
            "374     7.0  another gears game. A 9 for visuals, 6 for sto...\n",
            "\n",
            "[375 rows x 2 columns]\n",
            "10.0    136\n",
            "9.0      67\n",
            "8.0      60\n",
            "7.0      32\n",
            "0.0      25\n",
            "6.0      15\n",
            "5.0      15\n",
            "4.0       7\n",
            "1.0       6\n",
            "2.0       6\n",
            "3.0       6\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6o1BHhDcxvr"
      },
      "source": [
        "# Testing Dataset: Ghost of Tsushima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ga6LMrlc05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e6619f9-e2e4-4c5a-888c-eb4e7dea711d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/ghost-of-tsushima/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df23 = pd.DataFrame(review_dict)\n",
        "    df23 = df23.drop(columns=['name', 'date'])\n",
        "    df23.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/GhostT_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(75), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n",
            "[*] Working on page: 42\n",
            "[*] Working on page: 43\n",
            "[*] Working on page: 44\n",
            "[*] Working on page: 45\n",
            "[*] Working on page: 46\n",
            "[*] Working on page: 47\n",
            "[*] Working on page: 48\n",
            "[*] Working on page: 49\n",
            "[*] Working on page: 50\n",
            "[*] Working on page: 51\n",
            "[*] Working on page: 52\n",
            "[*] Working on page: 53\n",
            "[*] Working on page: 54\n",
            "[*] Working on page: 55\n",
            "[*] Working on page: 56\n",
            "[*] Working on page: 57\n",
            "[*] Working on page: 58\n",
            "[*] Working on page: 59\n",
            "[*] Working on page: 60\n",
            "[*] Working on page: 61\n",
            "[*] Working on page: 62\n",
            "[*] Working on page: 63\n",
            "[*] Working on page: 64\n",
            "[*] Working on page: 65\n",
            "[*] Working on page: 66\n",
            "[*] Working on page: 67\n",
            "[*] Working on page: 68\n",
            "[*] Working on page: 69\n",
            "[*] Working on page: 70\n",
            "[*] Working on page: 71\n",
            "[*] Working on page: 72\n",
            "[*] Working on page: 73\n",
            "[*] Working on page: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CZjg2c7drHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "55491d08-3e41-4a8f-fcbb-3205ec430e0f"
      },
      "source": [
        "df23 = pd.read_csv(\"drive/My Drive/csv's/MetacriticDataset/GhostT_reviews.csv\",engine='python' )\n",
        "df23 = df23.dropna()\n",
        "del df23['Unnamed: 0']\n",
        "df23 = df23.reset_index(drop=True)\n",
        "print(df23)\n",
        "print(df23['rating'].value_counts())\n",
        "df23.to_csv(r\"drive/My Drive/csv's/MetacriticDataset/GhostT_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  Amazing to see what Sucker Punch has achieved ...\n",
            "1       10.0  Very ambitious game from Sucker punch the deve...\n",
            "2       10.0  The game is purely amazing. Fun combat, a huge...\n",
            "3       10.0  Absolutely loving this game! Nice and fresh ta...\n",
            "4       10.0  The level Design os amazing. History and death...\n",
            "...      ...                                                ...\n",
            "6466     9.0  Just finished the game, beautiful story, loved...\n",
            "6467     9.0  ====================IIIIIIIIII GAME RANK : 8.5...\n",
            "6468     5.0  An absolute blast to play. Suckerpunch has rea...\n",
            "6469    10.0  This and the Witcher 3 are my top games for th...\n",
            "6470    10.0  Cool game nice,gameplay,graphics and Japan awe...\n",
            "\n",
            "[6471 rows x 2 columns]\n",
            "10.0    4547\n",
            "9.0     1294\n",
            "8.0      258\n",
            "0.0      105\n",
            "7.0       80\n",
            "5.0       45\n",
            "6.0       39\n",
            "4.0       34\n",
            "1.0       31\n",
            "3.0       19\n",
            "2.0       19\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJYGdo08U_K"
      },
      "source": [
        "# **Best 25 PS4 User Reviews:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3KEuHxm8fVw"
      },
      "source": [
        "The top 25 reviews (that have sufficient reviews) are collected to test for bias:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6dHlkRm8skI"
      },
      "source": [
        "# 1) Ghost of Tsushima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Y571HG8xKo"
      },
      "source": [
        "Already scraped above ^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogdJ24dK9Eor"
      },
      "source": [
        "# 2) Witcher 3: Wild Hunt "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgwUA98P6yK6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "89c62bce-12d3-46cc-c751-76ea454a67d1"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-witcher-3-wild-hunt/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df24 = pd.DataFrame(review_dict)\n",
        "    df24 = df24.drop(columns=['name', 'date'])\n",
        "    df24.to_csv(r\"drive/My Drive/csv's/Best/W3_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(20), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4l7ZC_B9ZQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "2a6cb303-a393-41a5-e132-7af37521a6ee"
      },
      "source": [
        "df24 = pd.read_csv(\"drive/My Drive/csv's/Best/W3_reviews.csv\",engine='python' )\n",
        "df24 = df24.dropna()\n",
        "del df24['Unnamed: 0']\n",
        "df24 = df24.reset_index(drop=True)\n",
        "print(df24)\n",
        "print(df24['rating'].value_counts())\n",
        "df24.to_csv(r\"drive/My Drive/csv's/Best/W3_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  Very awesome game, reminds me of Red Dead Rede...\n",
            "1       10.0  This is one of the best RPG and Open World gam...\n",
            "2       10.0  Amazing game with great gameplay and story. Th...\n",
            "3       10.0  This will go down as one of the best rpg's eve...\n",
            "4       10.0  Best RPG game ever ! Large diverse open world ...\n",
            "...      ...                                                ...\n",
            "1821    10.0  El mundo del juego es inmersivo, la historia t...\n",
            "1822     1.0  Купил игру по скидке, думал что понравится так...\n",
            "1823    10.0  The only thing that needs to be said about thi...\n",
            "1824    10.0  I played this game five times and planing to m...\n",
            "1825    10.0  The witcher 3 can easily be the best game of t...\n",
            "\n",
            "[1826 rows x 2 columns]\n",
            "10.0    1236\n",
            "9.0      270\n",
            "8.0       76\n",
            "7.0       60\n",
            "6.0       49\n",
            "0.0       47\n",
            "5.0       28\n",
            "1.0       17\n",
            "2.0       16\n",
            "3.0       14\n",
            "4.0       13\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lFGi97S9jzo"
      },
      "source": [
        "# 3) The Last of Us Remastered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8F-sdL_9oHR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "6f6bd89b-619b-4228-8fbf-f8d635622b09"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-last-of-us-remastered/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df25 = pd.DataFrame(review_dict)\n",
        "    df25 = df25.drop(columns=['name', 'date'])\n",
        "    df25.to_csv(r\"drive/My Drive/csv's/Best/TLOUR_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(26), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxLSuLnN9xVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "3f53f35f-0082-4d29-95ef-a92c8bd811aa"
      },
      "source": [
        "df25 = pd.read_csv(\"drive/My Drive/csv's/Best/TLOUR_reviews.csv\",engine='python' )\n",
        "df25 = df25.dropna()\n",
        "del df25['Unnamed: 0']\n",
        "df25 = df25.reset_index(drop=True)\n",
        "print(df25)\n",
        "print(df25['rating'].value_counts())\n",
        "df25.to_csv(r\"drive/My Drive/csv's/Best/TLOUR_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  Вместо тысяча слов , я скажу что это лучшая иг...\n",
            "1       10.0  This game is a 10. I don't believe that a game...\n",
            "2       10.0  Amazing Game, Perfect history, and a good game...\n",
            "3       10.0  The Way its meant to be play ...\\rThe last of ...\n",
            "4       10.0  I've been a PC gamer for the majority of my li...\n",
            "...      ...                                                ...\n",
            "2410    10.0  I'm super late to the party on this one, I kno...\n",
            "2411    10.0  Returning to this game hurts because you see h...\n",
            "2412    10.0  Epic game, a classic masterpiece, a must have ...\n",
            "2413    10.0  el mejor juego que haya existido 10/10, la his...\n",
            "2414    10.0  Best game ever you can play pls play this and ...\n",
            "\n",
            "[2415 rows x 2 columns]\n",
            "10.0    1733\n",
            "9.0      347\n",
            "8.0      122\n",
            "0.0       62\n",
            "7.0       45\n",
            "6.0       26\n",
            "5.0       23\n",
            "1.0       19\n",
            "3.0       15\n",
            "4.0       14\n",
            "2.0        9\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuJErwFB9xxo"
      },
      "source": [
        "# 4) God of War"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye3BGIxp919Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "71a46368-dbe5-42fd-bc23-9c913c97c1d1"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/god-of-war/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df26 = pd.DataFrame(review_dict)\n",
        "    df26 = df26.drop(columns=['name', 'date'])\n",
        "    df26.to_csv(r\"drive/My Drive/csv's/Best/GOW_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(42), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4psjRLe96nA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "1bf358d4-828b-4f52-fe95-1f59d3fc2960"
      },
      "source": [
        "df26 = pd.read_csv(\"drive/My Drive/csv's/Best/GOW_reviews.csv\",engine='python' )\n",
        "df26 = df26.dropna()\n",
        "del df26['Unnamed: 0']\n",
        "df26 = df26.reset_index(drop=True)\n",
        "print(df26)\n",
        "print(df26['rating'].value_counts())\n",
        "df26.to_csv(r\"drive/My Drive/csv's/Best/GOW_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        5.0  As a huge fan of the God of War series and The...\n",
            "1        4.0  Yes, the graphics are stunning and the storyli...\n",
            "2        4.0  Overrated but still fun. Just be prepared to g...\n",
            "3        4.0  Do you like QTE's? How about unskippable cutsc...\n",
            "4        3.0  I really wanted to like this game...but I don'...\n",
            "...      ...                                                ...\n",
            "3663     9.0  This was my first God of War game I have ever ...\n",
            "3664    10.0                     My favourite game. Masterpiece\n",
            "3665     9.0  ====================IIIIIIIIII GAME RANK : 9.5...\n",
            "3666     9.0  The best word to describe this game is \"grande...\n",
            "3667    10.0  This was an excellent reinvention for the God ...\n",
            "\n",
            "[3668 rows x 2 columns]\n",
            "10.0    2911\n",
            "9.0      315\n",
            "8.0      126\n",
            "0.0       73\n",
            "7.0       67\n",
            "6.0       47\n",
            "5.0       34\n",
            "4.0       32\n",
            "3.0       23\n",
            "2.0       21\n",
            "1.0       19\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q10c_IqC98Gy"
      },
      "source": [
        "# 5) SpongeBob SquarePants: Battle for Bikini Bottom - Rehydrated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQgwIsuB9_6n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5b26f621-1555-435d-f9e9-8600377e7d5f"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/spongebob-squarepants-battle-for-bikini-bottom---rehydrated/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df27 = pd.DataFrame(review_dict)\n",
        "    df27 = df27.drop(columns=['name', 'date'])\n",
        "    df27.to_csv(r\"drive/My Drive/csv's/Best/SBSP_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(11), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKmkEMi5-AlJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "403c20ea-016c-48d9-8cad-6bb5897970b1"
      },
      "source": [
        "df27 = pd.read_csv(\"drive/My Drive/csv's/Best/SBSP_reviews.csv\",engine='python' )\n",
        "df27 = df27.dropna()\n",
        "del df27['Unnamed: 0']\n",
        "df27 = df27.reset_index(drop=True)\n",
        "print(df27)\n",
        "print(df27['rating'].value_counts())\n",
        "df27.to_csv(r\"drive/My Drive/csv's/Best/SBSP_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  ‘Spongebob Squarepants: Battle For Bikini Bott...\n",
            "1      10.0  ntro: Painty the Pirate]\\rAre you ready, kids?...\n",
            "2      10.0  GAMESPOT ПОШЁЛ НАХУЙ!   WE GOT THIS COVERED ПО...\n",
            "3      10.0  The one of de best's plataform's 3D in 2000's,...\n",
            "4      10.0  ради того, чтобы оценить этот шедевр, я зареги...\n",
            "..      ...                                                ...\n",
            "696     5.0  This is simply a remake of a 2003 game.  it lo...\n",
            "697    10.0  This game is so fun . And loveable . I don't k...\n",
            "698    10.0   Определённо уровень Кодзимы,а может даже и выше.\n",
            "699     9.0  I'm scratching my head wondering if the game I...\n",
            "700     0.0  great game but too many bugs to care about i a...\n",
            "\n",
            "[701 rows x 2 columns]\n",
            "10.0    530\n",
            "9.0      60\n",
            "0.0      42\n",
            "8.0      34\n",
            "7.0      11\n",
            "5.0       7\n",
            "3.0       6\n",
            "1.0       4\n",
            "6.0       3\n",
            "4.0       3\n",
            "2.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDs7_dii-BTQ"
      },
      "source": [
        "# 6) Resident Evil 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sT0EKG0-EPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "8d42fec7-86b9-457d-9912-c1ffbf573918"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/resident-evil-2/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df28 = pd.DataFrame(review_dict)\n",
        "    df28 = df28.drop(columns=['name', 'date'])\n",
        "    df28.to_csv(r\"drive/My Drive/csv's/Best/RE2_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(9), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZyVPnZ9-HOI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "36d05ab5-fe4f-4028-fd5c-b6445160b3fc"
      },
      "source": [
        "df28 = pd.read_csv(\"drive/My Drive/csv's/Best/RE2_reviews.csv\",engine='python' )\n",
        "df28 = df28.dropna()\n",
        "del df28['Unnamed: 0']\n",
        "df28 = df28.reset_index(drop=True)\n",
        "print(df28)\n",
        "print(df28['rating'].value_counts())\n",
        "df28.to_csv(r\"drive/My Drive/csv's/Best/RE2_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Omg, this game is absolutely amazing and aweso...\n",
            "1      10.0  BEST GAME EVER!!!!! Thats all I have got to sa...\n",
            "2      10.0                                        I loved it.\n",
            "3      10.0  This has to be the best re-imagining in the se...\n",
            "4      10.0  One of my favorite game has revived. It is an ...\n",
            "..      ...                                                ...\n",
            "704     8.0  Game so far is pretty dope. As in the rest of ...\n",
            "705     9.0  Es lo mejor que he jugado en PS4, los unicos e...\n",
            "706    10.0  É um ótimo jogo! Extremamente bonito graficame...\n",
            "707     8.0  A great remake with engaging gameplay, immersi...\n",
            "708     9.0  Very good remake. Idea and implementation on h...\n",
            "\n",
            "[709 rows x 2 columns]\n",
            "10.0    404\n",
            "9.0     175\n",
            "8.0      60\n",
            "7.0      19\n",
            "0.0      11\n",
            "6.0       9\n",
            "4.0       8\n",
            "1.0       7\n",
            "5.0       6\n",
            "3.0       6\n",
            "2.0       4\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M8Cfl9A-HnB"
      },
      "source": [
        "# 7) Astro Bot: Rescue Mission "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQbSCoH5-LQx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b7b8dfed-f177-4c6c-faff-c75802438a18"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/astro-bot-rescue-mission/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df29 = pd.DataFrame(review_dict)\n",
        "    df29 = df29.drop(columns=['name', 'date'])\n",
        "    df29.to_csv(r\"drive/My Drive/csv's/Best/AB_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de0rfGn2-La7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "51ab9ce7-b3a6-4c97-c829-aa92d8cf280d"
      },
      "source": [
        "df29 = pd.read_csv(\"drive/My Drive/csv's/Best/AB_reviews.csv\",engine='python' )\n",
        "df29 = df29.dropna()\n",
        "del df29['Unnamed: 0']\n",
        "df29 = df29.reset_index(drop=True)\n",
        "print(df29)\n",
        "print(df29['rating'].value_counts())\n",
        "df29.to_csv(r\"drive/My Drive/csv's/Best/AB_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  This game is absolutely incredible. Having bee...\n",
            "1      10.0  Astro Bot Rescue Mission is a superb game; the...\n",
            "2      10.0  Absolutely fantastic game, this sets a new sta...\n",
            "3      10.0  This game is phenomenal. I've been looking for...\n",
            "4      10.0  This game is a must buy! Best looking game for...\n",
            "..      ...                                                ...\n",
            "132    10.0  Simplemente maravilloso,una aventura tierna,bu...\n",
            "133    10.0  One of the best Virtual Reality games I have e...\n",
            "134    10.0  When RE7 is to scary for you to play in VR tha...\n",
            "135    10.0  Don't hesitate, just buy it. It's an absolute ...\n",
            "136     9.0  Best game for VR. Funny, shows different VR po...\n",
            "\n",
            "[137 rows x 2 columns]\n",
            "10.0    117\n",
            "9.0      14\n",
            "0.0       3\n",
            "8.0       3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP1nXl1C-QqP"
      },
      "source": [
        "# 8) NieR: Automata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBhzhwXR-TvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "467704ba-b88a-469c-ac45-ec8e69956245"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/nier-automata/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "       review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df30 = pd.DataFrame(review_dict)\n",
        "    df30 = df30.drop(columns=['name', 'date'])\n",
        "    df30.to_csv(r\"drive/My Drive/csv's/Best/NA_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(9), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVOOiJEB-WKP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "f5cd3990-3860-4a1f-fdcd-3fc2c95cb252"
      },
      "source": [
        "df30 = pd.read_csv(\"drive/My Drive/csv's/Best/NA_reviews.csv\",engine='python' )\n",
        "df30 = df30.dropna()\n",
        "del df30['Unnamed: 0']\n",
        "df30 = df30.reset_index(drop=True)\n",
        "print(df30)\n",
        "print(df30['rating'].value_counts())\n",
        "df30.to_csv(r\"drive/My Drive/csv's/Best/NA_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  A really beautiful game that surpass my expect...\n",
            "1      10.0  You will have to buy and play this game by you...\n",
            "2      10.0  THE game that make you CRY after the true ending.\n",
            "3      10.0  BEST GAME OF THE YEAR. Story, sound, music, an...\n",
            "4      10.0  NieR: Automata is no doubtedly the most fantas...\n",
            "..      ...                                                ...\n",
            "698    10.0  Best game i ever played, the gun goes brrr, th...\n",
            "699    10.0  An absolute masterpiece, an absolute must-play...\n",
            "700    10.0  For me is a Beautiful game. I love all about t...\n",
            "701    10.0  Taro Yoko's amazing story telling, it says it ...\n",
            "702     9.0  I really enjoyed played this game, I just disa...\n",
            "\n",
            "[703 rows x 2 columns]\n",
            "10.0    454\n",
            "9.0     105\n",
            "8.0      47\n",
            "7.0      19\n",
            "6.0      19\n",
            "0.0      18\n",
            "5.0      16\n",
            "2.0       7\n",
            "4.0       7\n",
            "3.0       6\n",
            "1.0       5\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sEsgVM3-WbX"
      },
      "source": [
        "# 9) Bloodborne "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boLrBspf-YjQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "8e8ec0d6-7e45-4d75-d4ce-4e219b84e275"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/bloodborne/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df31 = pd.DataFrame(review_dict)\n",
        "    df31 = df31.drop(columns=['name', 'date'])\n",
        "    df31.to_csv(r\"drive/My Drive/csv's/Best/BB_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(21), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgMuPF9I-czn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "1de72ca1-ad07-45b7-c712-f885e3a0bf9d"
      },
      "source": [
        "df31 = pd.read_csv(\"drive/My Drive/csv's/Best/BB_reviews.csv\",engine='python' )\n",
        "df31 = df31.dropna()\n",
        "del df31['Unnamed: 0']\n",
        "df31 = df31.reset_index(drop=True)\n",
        "print(df31)\n",
        "print(df31['rating'].value_counts())\n",
        "df31.to_csv(r\"drive/My Drive/csv's/Best/BB_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  This is the best game you will get on any next...\n",
            "1       10.0  Bloodborne is as glorious as it is grotesque, ...\n",
            "2       10.0  I've never played DS/DS2. That said, this game...\n",
            "3       10.0  Beautiful game, coming by From Software, I exp...\n",
            "4       10.0  Do you like the Souls games? Buy this immediat...\n",
            "...      ...                                                ...\n",
            "1923    10.0  Um jogo simplesmente incrível, uma evolução id...\n",
            "1924    10.0  My favourite From game! Takes the formula and ...\n",
            "1925     0.0  **** this game and **** from software !!!!!!!!...\n",
            "1926    10.0  This became quickly my all time favorite, and ...\n",
            "1927    10.0  Um dos melhores jogos de todos os tempos, obri...\n",
            "\n",
            "[1928 rows x 2 columns]\n",
            "10.0    1172\n",
            "9.0      387\n",
            "8.0      105\n",
            "0.0       59\n",
            "7.0       58\n",
            "6.0       31\n",
            "5.0       31\n",
            "1.0       23\n",
            "4.0       22\n",
            "3.0       22\n",
            "2.0       18\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVU9AKf-dHf"
      },
      "source": [
        "# 10) Detroit: Become Human "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgdr6Lv-gwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "77c8063e-ce13-48f6-928c-51e92f4be367"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/detroit-become-human/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df32 = pd.DataFrame(review_dict)\n",
        "    df32 = df32.drop(columns=['name', 'date'])\n",
        "    df32.to_csv(r\"drive/My Drive/csv's/Best/DBH_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(12), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyOHtQPl-hw5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "6b64c6fc-81c3-4696-d9fa-cafb8ac4d22f"
      },
      "source": [
        "df32 = pd.read_csv(\"drive/My Drive/csv's/Best/DBH_reviews.csv\",engine='python' )\n",
        "df32 = df32.dropna()\n",
        "del df32['Unnamed: 0']\n",
        "df32 = df32.reset_index(drop=True)\n",
        "print(df32)\n",
        "print(df32['rating'].value_counts())\n",
        "df32.to_csv(r\"drive/My Drive/csv's/Best/DBH_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  Ignore the hate-reviews with a personal agenda...\n",
            "1       10.0  I'm absolutely blown away by this game. The gr...\n",
            "2       10.0  I have been playing it since it came out, and ...\n",
            "3       10.0  I'm a huge fan of David Cage games and I gotta...\n",
            "4       10.0  The detail, voice acting, the music is incredi...\n",
            "...      ...                                                ...\n",
            "1008     2.0  I had to rate a 2 instead of 3 for one reason,...\n",
            "1009    10.0  El mejor juego que jugué en mi vida, historia ...\n",
            "1010    10.0  Начнём с хорошего... Графика - супер, захват л...\n",
            "1011     0.0  Awful, absolutely terrible. This is one of the...\n",
            "1012     9.0  ====================IIIIIIIIII GAME RANK : 9.0...\n",
            "\n",
            "[1013 rows x 2 columns]\n",
            "10.0    572\n",
            "9.0     217\n",
            "8.0      96\n",
            "7.0      41\n",
            "0.0      26\n",
            "5.0      18\n",
            "6.0      17\n",
            "3.0      10\n",
            "2.0       6\n",
            "4.0       5\n",
            "1.0       5\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES5kCab5-iE_"
      },
      "source": [
        "# 11) Dark Souls III"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivz0zxb7-kV3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "dc4d4f75-e20c-4c80-f965-70731af1497c"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/dark-souls-iii/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df33 = pd.DataFrame(review_dict)\n",
        "    df33 = df33.drop(columns=['name', 'date'])\n",
        "    df33.to_csv(r\"drive/My Drive/csv's/Best/DS3_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(6), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J6q1TqV-kln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "7d99ac33-d20e-4a2d-d9f9-00f5c27bbda2"
      },
      "source": [
        "df33 = pd.read_csv(\"drive/My Drive/csv's/Best/DS3_reviews.csv\",engine='python' )\n",
        "df33 = df33.dropna()\n",
        "del df33['Unnamed: 0']\n",
        "df33 = df33.reset_index(drop=True)\n",
        "print(df33)\n",
        "print(df33['rating'].value_counts())\n",
        "df33.to_csv(r\"drive/My Drive/csv's/Best/DS3_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Just started playing this last night, already ...\n",
            "1       6.0  I'm a diehard Sun Bro and have been Praisin' f...\n",
            "2       5.0  It's the best game ever since the one that was...\n",
            "3      10.0  It's really early to say but this game is just...\n",
            "4      10.0  What an amazing game, 5 hours in, fought that ...\n",
            "..      ...                                                ...\n",
            "459     9.0  Wonderful world Wonderful music Wonderful figh...\n",
            "460    10.0  Моя любимая игра в серии, уступает лишь бладбо...\n",
            "461     9.0  The best Dark Souls game for me. This game has...\n",
            "462     8.0  This game didn't instantly grab me, but the mo...\n",
            "463     9.0  One of the best games ever made.It's include a...\n",
            "\n",
            "[464 rows x 2 columns]\n",
            "10.0    198\n",
            "9.0     130\n",
            "8.0      53\n",
            "7.0      25\n",
            "0.0      19\n",
            "4.0      10\n",
            "6.0       9\n",
            "3.0       7\n",
            "5.0       6\n",
            "1.0       4\n",
            "2.0       3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCVW9lZ3-j5n"
      },
      "source": [
        "# 12) Dreams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuRrslDO-p5H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9692cd49-abeb-4c57-d060-b4d26d7708d4"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/dreams/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df34 = pd.DataFrame(review_dict)\n",
        "    df34 = df34.drop(columns=['name', 'date'])\n",
        "    df34.to_csv(r\"drive/My Drive/csv's/Best/DREAMS_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(6), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgfG9mXp-qgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "83ffd797-1592-4d94-edb3-6403a3c564ef"
      },
      "source": [
        "df34 = pd.read_csv(\"drive/My Drive/csv's/Best/DREAMS_reviews.csv\",engine='python' )\n",
        "df34 = df34.dropna()\n",
        "del df34['Unnamed: 0']\n",
        "df34 = df34.reset_index(drop=True)\n",
        "print(df34)\n",
        "print(df34['rating'].value_counts())\n",
        "df34.to_csv(r\"drive/My Drive/csv's/Best/DREAMS_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Dreams is an incredible creativity tool, it ha...\n",
            "1      10.0  Dreams is a definive tool, for making leves, a...\n",
            "2      10.0  Since day one of early access, I have spent mo...\n",
            "3      10.0  Been playing consecutively since the first bet...\n",
            "4      10.0  Incredible game. Amazing campaign. Outstanding...\n",
            "..      ...                                                ...\n",
            "472     9.0  Es el mejor juego de la historia de ps4. La hi...\n",
            "473     8.0  I am happy that dreams does exist, It is uniqu...\n",
            "474     9.0  Very good concept. Really enjoyed using my ima...\n",
            "475    10.0  This game has Tower Ball! This is the best gam...\n",
            "476    10.0  Iuuuuibfb Betty ven jakbwbwbwddbwbwansnbsnsbsb...\n",
            "\n",
            "[477 rows x 2 columns]\n",
            "10.0    380\n",
            "9.0      39\n",
            "1.0       8\n",
            "5.0       8\n",
            "0.0       8\n",
            "7.0       8\n",
            "8.0       7\n",
            "4.0       6\n",
            "6.0       6\n",
            "3.0       4\n",
            "2.0       3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h7oZX2T-q9S"
      },
      "source": [
        "# 13) Marvel's Spider-Man"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LYdx9IF-uDv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "3a8496ac-8124-4772-ddd7-e4a09ea14d12"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/marvels-spider-man/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df35 = pd.DataFrame(review_dict)\n",
        "    df35 = df35.drop(columns=['name', 'date'])\n",
        "    df35.to_csv(r\"drive/My Drive/csv's/Best/SM_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(17), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbt7hLC7-uNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "7c82231d-25dd-4e7c-c817-c24996f4e635"
      },
      "source": [
        "df35 = pd.read_csv(\"drive/My Drive/csv's/Best/SM_reviews.csv\",engine='python' )\n",
        "df35 = df35.dropna()\n",
        "del df35['Unnamed: 0']\n",
        "df35 = df35.reset_index(drop=True)\n",
        "print(df35)\n",
        "print(df35['rating'].value_counts())\n",
        "df35.to_csv(r\"drive/My Drive/csv's/Best/SM_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        8.0  Это лучшая игра про человека-паука , поиграйте...\n",
            "1        9.0  Great gameplay, great graphics,  good story an...\n",
            "2       10.0  Pros: Gorgeous game especially on the pro. Dra...\n",
            "3       10.0  What a beautifully crafted game, graphics, gam...\n",
            "4       10.0  I'm not an avid gamer and this was my first ti...\n",
            "...      ...                                                ...\n",
            "1540    10.0  Spider-Man is a splendidly tuned action game w...\n",
            "1541    10.0  No sloilers! I like games with open world (min...\n",
            "1542    10.0  Awesome game! Fun combat, amazing web slinging...\n",
            "1543     9.0  good. veery good. maybe even the best of all g...\n",
            "1544     9.0  Very good game, graphics, feelings, content. M...\n",
            "\n",
            "[1545 rows x 2 columns]\n",
            "10.0    690\n",
            "9.0     475\n",
            "8.0     194\n",
            "7.0      77\n",
            "6.0      26\n",
            "0.0      25\n",
            "5.0      20\n",
            "4.0      13\n",
            "3.0      12\n",
            "1.0       7\n",
            "2.0       6\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSVEkK1Z-vKP"
      },
      "source": [
        "# 14) Dragon Quest XI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PB8nNWZ-xwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "021a881d-eb40-4919-eca4-6bf0c979df5d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/dragon-quest-xi-echoes-of-an-elusive-age/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df36 = pd.DataFrame(review_dict)\n",
        "    df36 = df36.drop(columns=['name', 'date'])\n",
        "    df36.to_csv(r\"drive/My Drive/csv's/Best/DQ_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYn6jR3o-x_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "7f5db0ef-b9a5-40ca-8856-6ce354a95ddc"
      },
      "source": [
        "df36 = pd.read_csv(\"drive/My Drive/csv's/Best/DQ_reviews.csv\",engine='python' )\n",
        "df36 = df36.dropna()\n",
        "del df36['Unnamed: 0']\n",
        "df36 = df36.reset_index(drop=True)\n",
        "print(df36)\n",
        "print(df36['rating'].value_counts())\n",
        "df36.to_csv(r\"drive/My Drive/csv's/Best/DQ_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Last year the best JRPG on the PS4 was Persona...\n",
            "1      10.0  great and classic game I love it so much.this ...\n",
            "2      10.0  Dragon Quest XI is one of the best video games...\n",
            "3      10.0  I was expecting a good JRPG, that would bring ...\n",
            "4      10.0  at last the real rpg game whe deserve, dragon ...\n",
            "..      ...                                                ...\n",
            "180     8.0  I give it a 8 only because the music wasn’t th...\n",
            "181     9.0  Really great game. Does everything you ask for...\n",
            "182    10.0  Nowadays, while RPG now shifting toward action...\n",
            "183     6.0  This is one of those games that I loved the fi...\n",
            "184    10.0  It’s my first dq game and im love it the story...\n",
            "\n",
            "[185 rows x 2 columns]\n",
            "10.0    96\n",
            "9.0     36\n",
            "8.0     16\n",
            "7.0     13\n",
            "0.0      8\n",
            "5.0      5\n",
            "6.0      4\n",
            "4.0      3\n",
            "2.0      2\n",
            "3.0      1\n",
            "1.0      1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMFDhTvz-zUP"
      },
      "source": [
        "# 15) Persona 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJRHW834-1So",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "f6648542-9425-4f1b-c809-2beb5d6b78bf"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/persona-5/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df37 = pd.DataFrame(review_dict)\n",
        "    df37 = df37.drop(columns=['name', 'date'])\n",
        "    df37.to_csv(r\"drive/My Drive/csv's/Best/P5_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(10), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF2v848m-1hQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d0771add-dc64-45db-da46-cfceaf81f536"
      },
      "source": [
        "df37 = pd.read_csv(\"drive/My Drive/csv's/Best/P5_reviews.csv\",engine='python' )\n",
        "df37 = df37.dropna()\n",
        "del df37['Unnamed: 0']\n",
        "df37 = df37.reset_index(drop=True)\n",
        "print(df37)\n",
        "print(df37['rating'].value_counts())\n",
        "df37.to_csv(r\"drive/My Drive/csv's/Best/P5_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       9.0  A great game. I loved P3 and P4 maybe a bit mo...\n",
            "1       9.0  Game could easily be a 10 but i don't think an...\n",
            "2       9.0  Best Persona game yet. I have played most of A...\n",
            "3      10.0  The game is a masterpiece of JRPG goodness. It...\n",
            "4       9.0  This is my first entry into the series. 113 ho...\n",
            "..      ...                                                ...\n",
            "892     9.0  Actual Score: 9.5/10Considering I’ve been unab...\n",
            "893    10.0  Oyandigim en iyi oyun, herhangi weeb oyunu buu...\n",
            "894     8.0  Good game. Good game. Good game. Good game. Go...\n",
            "895     8.0  Persona 5 is amazing but it's also extremely f...\n",
            "896     7.0  Finished the game for almost 250hrs. I dont li...\n",
            "\n",
            "[897 rows x 2 columns]\n",
            "10.0    622\n",
            "9.0     125\n",
            "8.0      47\n",
            "7.0      35\n",
            "6.0      15\n",
            "0.0      14\n",
            "1.0      11\n",
            "3.0       9\n",
            "5.0       9\n",
            "4.0       6\n",
            "2.0       4\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q01Kr9Ja-2bH"
      },
      "source": [
        "# 16) Life is Strange"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWM5mfuV-4KI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bdbdbde6-c4d6-4860-fcb2-67e9b003a7f0"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/life-is-strange/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find('span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df38 = pd.DataFrame(review_dict)\n",
        "    df38 = df38.drop(columns=['name', 'date'])\n",
        "    df38.to_csv(r\"drive/My Drive/csv's/Best/LIS_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4LVuEnh-4VP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b4e868a9-b0a7-4e6a-dd22-d75609b325a3"
      },
      "source": [
        "df38 = pd.read_csv(\"drive/My Drive/csv's/Best/LIS_reviews.csv\",engine='python' )\n",
        "df38 = df38.dropna()\n",
        "del df38['Unnamed: 0']\n",
        "df38 = df38.reset_index(drop=True)\n",
        "print(df38)\n",
        "print(df38['rating'].value_counts())\n",
        "df38.to_csv(r\"drive/My Drive/csv's/Best/LIS_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Further proof that video games can tell emotio...\n",
            "1       5.0  La idea de la historia y del juego esta bien, ...\n",
            "2       8.0                           Deutsche Review - Kritik\n",
            "3       7.0  An enjoyable teen drama. With its likable char...\n",
            "4       5.0  I know people will disagree with me but honest...\n",
            "..      ...                                                ...\n",
            "241    10.0  I really liked the Story. The worldbuilding wo...\n",
            "242    10.0  Life Is Strange is the best game ever, DONTNOD...\n",
            "243    10.0  Maravilloso de principio a fin... Se disfruta ...\n",
            "244    10.0  Ótimo jogo,traz uma experiência única,nós faz ...\n",
            "245     9.0  Great game what i love about this game is u ca...\n",
            "\n",
            "[246 rows x 2 columns]\n",
            "10.0    101\n",
            "9.0      59\n",
            "8.0      34\n",
            "7.0      28\n",
            "4.0       6\n",
            "6.0       6\n",
            "5.0       5\n",
            "1.0       2\n",
            "2.0       2\n",
            "3.0       2\n",
            "0.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sax9ArgV-9mg"
      },
      "source": [
        "# 17) A Plague Tale: Innocence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhynC3AI_Dno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4474c18a-4ac8-4a86-959e-71afca5905f8"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/a-plague-tale-innocence/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df39 = pd.DataFrame(review_dict)\n",
        "    df39 = df39.drop(columns=['name', 'date'])\n",
        "    df39.to_csv(r\"drive/My Drive/csv's/Best/PTI_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDIkRKh_EY3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "12a6941e-9bc2-4bce-d973-7650c57fcfef"
      },
      "source": [
        "df39 = pd.read_csv(\"drive/My Drive/csv's/Best/PTI_reviews.csv\",engine='python' )\n",
        "df39 = df39.dropna()\n",
        "del df39['Unnamed: 0']\n",
        "df39 = df39.reset_index(drop=True)\n",
        "print(df39)\n",
        "print(df39['rating'].value_counts())\n",
        "df39.to_csv(r\"drive/My Drive/csv's/Best/PTI_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  What can I say? Wow! I found out about this ga...\n",
            "1       9.0  Awesome and very Unique game with outstanding ...\n",
            "2       9.0  The game is beautiful and amazing it is a stor...\n",
            "3       7.0  Giving it a 7 as a sign of support. If you lik...\n",
            "4       6.0  I can see the developers put a LOT of love in ...\n",
            "..      ...                                                ...\n",
            "177     0.0  je trouve que tout dans ce jeux est bon, beau,...\n",
            "178     9.0  excelente juego no le tenia mucha fe, pero a l...\n",
            "179     9.0  Ce jeu est tellement bon que j'ai du m'imposer...\n",
            "180    10.0  This game is masterpiece . And most under rate...\n",
            "181    10.0  This game is masterpiece made by asobo studios...\n",
            "\n",
            "[182 rows x 2 columns]\n",
            "10.0    68\n",
            "9.0     53\n",
            "8.0     25\n",
            "7.0     12\n",
            "6.0     11\n",
            "5.0      4\n",
            "0.0      3\n",
            "1.0      2\n",
            "3.0      2\n",
            "2.0      1\n",
            "4.0      1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CB2-ESe_KPU"
      },
      "source": [
        "# 18) The Evil Within 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaIHJUXD_OQv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7249ce98-b365-4edd-c8bf-2d8409332552"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/the-evil-within-2/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df40 = pd.DataFrame(review_dict)\n",
        "    df40 = df40.drop(columns=['name', 'date'])\n",
        "    df40.to_csv(r\"drive/My Drive/csv's/Best/EW2_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NavwTKon_Odf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "3a170785-a731-456e-f1a6-bad2af866394"
      },
      "source": [
        "df40 = pd.read_csv(\"drive/My Drive/csv's/Best/EW2_reviews.csv\",engine='python' )\n",
        "df40 = df40.dropna()\n",
        "del df40['Unnamed: 0']\n",
        "df40 = df40.reset_index(drop=True)\n",
        "print(df40)\n",
        "print(df40['rating'].value_counts())\n",
        "df40.to_csv(r\"drive/My Drive/csv's/Best/EW2_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       9.0  That's a very good second entrance from Tango ...\n",
            "1      10.0  this game is big improvement about anything\\rt...\n",
            "2       9.0  Really good game, I was slightly worried when ...\n",
            "3      10.0  Wow, this game is amazing and incredible horro...\n",
            "4       7.0  The Evil Within 2 is a slightly above average ...\n",
            "..      ...                                                ...\n",
            "194     8.0  Una digna secuela de una gran primera parte, a...\n",
            "195     8.0  Um excelente jogo, possui uma história envolve...\n",
            "196     8.0  The graphics and gameplay are awesome, the sem...\n",
            "197    10.0  While The Evil Within 2 isn’t particularly sca...\n",
            "198    10.0  Super nice game from one of my favorite Horror...\n",
            "\n",
            "[199 rows x 2 columns]\n",
            "10.0    62\n",
            "8.0     49\n",
            "9.0     45\n",
            "7.0     12\n",
            "0.0      7\n",
            "4.0      7\n",
            "6.0      6\n",
            "3.0      4\n",
            "5.0      4\n",
            "2.0      2\n",
            "1.0      1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA_xgQKS_PqX"
      },
      "source": [
        "# 19) Titanfall 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q31Low8D_SUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "d1fda5cf-d1ae-479c-caf5-af5dd55fdec4"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/titanfall-2/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df41 = pd.DataFrame(review_dict)\n",
        "    df41 = df41.drop(columns=['name', 'date'])\n",
        "    df41.to_csv(r\"drive/My Drive/csv's/Best/TF2_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM-YQytu_SeP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9e54ee8c-8f61-4173-917f-f6c2eb2ee862"
      },
      "source": [
        "df41 = pd.read_csv(\"drive/My Drive/csv's/Best/TF2_reviews.csv\",engine='python' )\n",
        "df41 = df41.dropna()\n",
        "del df41['Unnamed: 0']\n",
        "df41 = df41.reset_index(drop=True)\n",
        "print(df41)\n",
        "print(df41['rating'].value_counts())\n",
        "df41.to_csv(r\"drive/My Drive/csv's/Best/TF2_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       9.0  Titanfall 2 has really surprised me. I was not...\n",
            "1       9.0  So far the campaign is pretty fun to play. It'...\n",
            "2      10.0  Simply the best. Fast, fun and furious. PS pla...\n",
            "3      10.0  Titanfall 2 is the most exciting shooter I hav...\n",
            "4      10.0  Fluid that's all that needs to be said about T...\n",
            "..      ...                                                ...\n",
            "347     7.0  The time I this game for first time it was har...\n",
            "348    10.0  Unlike al other fps shooters at the time and e...\n",
            "349    10.0  The story is great! And the movment too! Every...\n",
            "350     9.0  Where do I start?Campaign:If you're looking fo...\n",
            "351     0.0  The greatest FPS solo campaign I've ever playe...\n",
            "\n",
            "[352 rows x 2 columns]\n",
            "10.0    141\n",
            "9.0     122\n",
            "8.0      44\n",
            "7.0      20\n",
            "0.0       5\n",
            "6.0       5\n",
            "2.0       3\n",
            "3.0       3\n",
            "4.0       3\n",
            "1.0       3\n",
            "5.0       3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9li8yWLI_TsP"
      },
      "source": [
        "# 20) Uncharted 4: A Thief's End "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlB-7IHo_Vsn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "6b17c8f7-46b0-49fc-dae5-0eff1db0b91e"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/uncharted-4-a-thiefs-end/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df42 = pd.DataFrame(review_dict)\n",
        "    df42 = df42.drop(columns=['name', 'date'])\n",
        "    df42.to_csv(r\"drive/My Drive/csv's/Best/UC4_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(23), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q_h6pvP_V9X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9dae24cc-3596-489a-fbab-54f9f6b592ce"
      },
      "source": [
        "df42 = pd.read_csv(\"drive/My Drive/csv's/Best/UC4_reviews.csv\",engine='python' )\n",
        "df42 = df42.dropna()\n",
        "del df42['Unnamed: 0']\n",
        "df42 = df42.reset_index(drop=True)\n",
        "print(df42)\n",
        "print(df42['rating'].value_counts())\n",
        "df42.to_csv(r\"drive/My Drive/csv's/Best/UC4_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0       10.0  From Beginning to end Uncharted 4: A thief's e...\n",
            "1       10.0  Naughty Dog delivers again.  I played most of ...\n",
            "2       10.0  Damn, Damn, Damn what a game I love it, I love...\n",
            "3       10.0  The best of Uncharted franchise. The graphics ...\n",
            "4       10.0  you really no they are quicly made accounts ju...\n",
            "...      ...                                                ...\n",
            "2189    10.0  El juego increíble. Una pena que no lo pueda p...\n",
            "2190     8.0  Simple story, complex characters is essentiall...\n",
            "2191     8.0  Muy buen juego, mejor que los anteriores, graf...\n",
            "2192    10.0  Fechou a série com chave de ouro, foi um dos m...\n",
            "2193     9.0  ====================IIIIIIIIII GAME RANK : 9.0...\n",
            "\n",
            "[2194 rows x 2 columns]\n",
            "10.0    1395\n",
            "9.0      362\n",
            "8.0      161\n",
            "7.0      113\n",
            "6.0       43\n",
            "0.0       35\n",
            "5.0       29\n",
            "4.0       25\n",
            "1.0       16\n",
            "3.0       10\n",
            "2.0        5\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pMXrAqn_YNd"
      },
      "source": [
        "# 21) Rachet & Clank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zln28kKK_bYf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "58133588-ef8c-4af2-e9e8-4d3c99b468de"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/ratchet-clank/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df43 = pd.DataFrame(review_dict)\n",
        "    df43 = df43.drop(columns=['name', 'date'])\n",
        "    df43.to_csv(r\"drive/My Drive/csv's/Best/RC_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIumJyED_cMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "eb2ddf44-622d-4583-e309-4f542e8ede2e"
      },
      "source": [
        "df43 = pd.read_csv(\"drive/My Drive/csv's/Best/RC_reviews.csv\",engine='python' )\n",
        "df43 = df43.dropna()\n",
        "del df43['Unnamed: 0']\n",
        "df43 = df43.reset_index(drop=True)\n",
        "print(df43)\n",
        "print(df43['rating'].value_counts())\n",
        "df43.to_csv(r\"drive/My Drive/csv's/Best/RC_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       9.0  So, i'm a newcomer. Never got to experience pl...\n",
            "1      10.0  FANTASTIC game - it is so refreshing to see an...\n",
            "2      10.0  Okay, so initially I wasn't really excited for...\n",
            "3      10.0  this Ratchet and clank game is one of the best...\n",
            "4      10.0  I love this series, so I was thrilled to have ...\n",
            "..      ...                                                ...\n",
            "358     9.0  I loved the game from beginning to end. The gr...\n",
            "359     8.0  First game where 30 fps doesn't feel clunky, s...\n",
            "360     8.0  Ratchet and Clank is a culmination of everythi...\n",
            "361     8.0  the game is a good enought retelling but the c...\n",
            "362     8.0  Phenomenal all around game! Upgrade system of ...\n",
            "\n",
            "[363 rows x 2 columns]\n",
            "10.0    128\n",
            "9.0     107\n",
            "8.0      75\n",
            "7.0      27\n",
            "5.0      11\n",
            "6.0       7\n",
            "1.0       3\n",
            "4.0       3\n",
            "2.0       2\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIIFrCwd_e_I"
      },
      "source": [
        "# 22) Uncharted: The Nathan Drake Collection "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FJ3WmxG_heo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ed09c2ba-16b3-4f77-de45-48331bcce047"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/uncharted-the-nathan-drake-collection/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df44 = pd.DataFrame(review_dict)\n",
        "    df44 = df44.drop(columns=['name', 'date'])\n",
        "    df44.to_csv(r\"drive/My Drive/csv's/Best/UCC_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QXLNIUO_how",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "bb15e7be-d422-4300-ca80-1b69b2d25d03"
      },
      "source": [
        "df44 = pd.read_csv(\"drive/My Drive/csv's/Best/UCC_reviews.csv\",engine='python' )\n",
        "df44 = df44.dropna()\n",
        "del df44['Unnamed: 0']\n",
        "df44 = df44.reset_index(drop=True)\n",
        "print(df44)\n",
        "print(df44['rating'].value_counts())\n",
        "df44.to_csv(r\"drive/My Drive/csv's/Best/UCC_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  Feeling jealous listening my PS3 friends talki...\n",
            "1      10.0  It's funny to see how poor are these xbox/nint...\n",
            "2      10.0  It's brilliant that Bluepoint remastered these...\n",
            "3      10.0  I don't wanna say long. Why? I feel I don't ha...\n",
            "4      10.0  3 Great games looking better than ever, especi...\n",
            "..      ...                                                ...\n",
            "333    10.0  Отличная серия. Самая лучшая как по мне 3 част...\n",
            "334    10.0              It's a  very injoyable set of game's.\n",
            "335    10.0  Feels like playing a mix of Mission Impossible...\n",
            "336     9.0  Just created my account and wanted to come rev...\n",
            "337     7.0  I never played the Uncharted Series until well...\n",
            "\n",
            "[338 rows x 2 columns]\n",
            "10.0    133\n",
            "9.0      85\n",
            "8.0      56\n",
            "7.0      15\n",
            "0.0      14\n",
            "6.0      12\n",
            "5.0      11\n",
            "4.0       8\n",
            "1.0       2\n",
            "2.0       1\n",
            "3.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyCmTv3_kIv"
      },
      "source": [
        "# 23) Rocket League "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS4SWaok_meI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "56038a96-a0dc-4f4e-a29c-d6da0d32a16d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/rocket-league/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df45 = pd.DataFrame(review_dict)\n",
        "    df45 = df45.drop(columns=['name', 'date'])\n",
        "    df45.to_csv(r\"drive/My Drive/csv's/Best/RL_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or-hkKAo_moC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "57ccb9b2-6b28-417b-f9b0-3869d73094d7"
      },
      "source": [
        "df45 = pd.read_csv(\"drive/My Drive/csv's/Best/RL_reviews.csv\",engine='python' )\n",
        "df45 = df45.dropna()\n",
        "del df45['Unnamed: 0']\n",
        "df45 = df45.reset_index(drop=True)\n",
        "print(df45)\n",
        "print(df45['rating'].value_counts())\n",
        "df45.to_csv(r\"drive/My Drive/csv's/Best/RL_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  An incredible game created by incredible devel...\n",
            "1       9.0  Absolutely in love with this game. The learnin...\n",
            "2      10.0  Ok, I'll be the first to admit, I didn't think...\n",
            "3      10.0  I dont like football game and races, bat this ...\n",
            "4      10.0  Psyonix, the developer of Rocket League had pr...\n",
            "..      ...                                                ...\n",
            "347    10.0  È molto divertente da giocare con gli amici, p...\n",
            "348    10.0  I was hesitating about buying this game, and n...\n",
            "349     8.0  hhohohohoohohohhohohohohoohohohohohohohohohoho...\n",
            "350     4.0  the concept of the game is not so bad but the ...\n",
            "351     7.0  • Extremely Fun To Play Solo Or With Friends!•...\n",
            "\n",
            "[352 rows x 2 columns]\n",
            "10.0    174\n",
            "9.0      91\n",
            "8.0      49\n",
            "7.0      16\n",
            "0.0       9\n",
            "5.0       6\n",
            "6.0       3\n",
            "1.0       2\n",
            "4.0       1\n",
            "2.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJNf0IA_nUz"
      },
      "source": [
        "# 24) Devil May Cry 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKTtS7uD_qS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "50deb0f9-118c-4d22-e4d2-5d950a96411d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/devil-may-cry-5/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df46 = pd.DataFrame(review_dict)\n",
        "    df46 = df46.drop(columns=['name', 'date'])\n",
        "    df46.to_csv(r\"drive/My Drive/csv's/Best/DMC_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma6QSxuV_wWP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "cf66ac9a-00ab-4340-fd0a-436fb6dc7f93"
      },
      "source": [
        "df46 = pd.read_csv(\"drive/My Drive/csv's/Best/DMC_reviews.csv\",engine='python' )\n",
        "df46 = df46.dropna()\n",
        "del df46['Unnamed: 0']\n",
        "df46 = df46.reset_index(drop=True)\n",
        "print(df46)\n",
        "print(df46['rating'].value_counts())\n",
        "df46.to_csv(r\"drive/My Drive/csv's/Best/DMC_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0      10.0  this game is absolutely worth to wait about 10...\n",
            "1       9.0  Hideaki Itsuno has been the director to some o...\n",
            "2       9.0  ok, so i will say this game has a very small a...\n",
            "3       8.0  I'm no DMC or action game expert (you can read...\n",
            "4       9.0  What does demons, Japan, metal music, and swor...\n",
            "..      ...                                                ...\n",
            "278    10.0  I have been a dmc fan since the first game and...\n",
            "279    10.0  This is the best fast-peace action game that I...\n",
            "280     0.0  I never played the game but I figured I'd give...\n",
            "281     8.0  Nice game play verry enjoe But verry fast to e...\n",
            "282    10.0  Amazing Hack n slash! Great graphics, people s...\n",
            "\n",
            "[283 rows x 2 columns]\n",
            "10.0    131\n",
            "9.0      69\n",
            "8.0      37\n",
            "7.0      12\n",
            "6.0      10\n",
            "0.0       6\n",
            "5.0       6\n",
            "4.0       5\n",
            "3.0       4\n",
            "1.0       2\n",
            "2.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ik1M1XO_xs7"
      },
      "source": [
        "# 25) Red Dead Redemption 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNjteGtS_1H3"
      },
      "source": [
        "Already have this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU0-FzHMgd5C"
      },
      "source": [
        "# **Worst 25 PS4 User Reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh8kvkm9gz3_"
      },
      "source": [
        "# 1) Madden NFL 21 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1DMex6HgkVp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "0a9aa184-d04e-4f4b-b186-dd5a04807907"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/madden-nfl-21/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df47 = pd.DataFrame(review_dict)\n",
        "    df47 = df47.drop(columns=['name', 'date'])\n",
        "    df47.to_csv(r\"drive/My Drive/csv's/Worst/NFL21_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9YsawBxgx6U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "2e46539b-bf61-4e8f-bccc-bb3702ea59f6"
      },
      "source": [
        "df47 = pd.read_csv(\"drive/My Drive/csv's/Worst/NFL21_reviews.csv\",engine='python' )\n",
        "df47 = df47.dropna()\n",
        "del df47['Unnamed: 0']\n",
        "df47 = df47.reset_index(drop=True)\n",
        "print(df47)\n",
        "print(df47['rating'].value_counts())\n",
        "df47.to_csv(r\"drive/My Drive/csv's/Worst/NFL21_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  This game is atrocious. Same game as Madden 20...\n",
            "1       0.0  An atrocious football simulation game. EA has ...\n",
            "2       0.0  This game is boring. Full of glitches. Also fr...\n",
            "3       0.0  This game is straight up unplayable, somehow t...\n",
            "4       0.0  Same game but actually WORSE. Gameplay is tras...\n",
            "..      ...                                                ...\n",
            "257     0.0  The gameplay, the menus, the announcing, it’s ...\n",
            "258     4.0  Myself and 10 others only buy this game for ou...\n",
            "259     0.0  Absolutely trash. EA needs to give up their li...\n",
            "260     2.0  Once again not worth I went against my gut fee...\n",
            "261     2.0  Same game as last year with no updates to fran...\n",
            "\n",
            "[262 rows x 2 columns]\n",
            "0.0     227\n",
            "1.0      17\n",
            "2.0       7\n",
            "4.0       4\n",
            "3.0       4\n",
            "10.0      2\n",
            "5.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVppA3RhObK"
      },
      "source": [
        "# 2) NBA 2K20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHHJCakdjR1A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "36fd62da-a1d2-4aee-c1dd-16728668ec34"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/nba-2k20/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df48 = pd.DataFrame(review_dict)\n",
        "    df48 = df48.drop(columns=['name', 'date'])\n",
        "    df48.to_csv(r\"drive/My Drive/csv's/Worst/NBA20_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(7), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K9XHoLFjQ2w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "c8405250-05a8-4b73-cd1c-cca25addc3b9"
      },
      "source": [
        "df48 = pd.read_csv(\"drive/My Drive/csv's/Worst/NBA20_reviews.csv\",engine='python' )\n",
        "df48 = df48.dropna()\n",
        "del df48['Unnamed: 0']\n",
        "df48 = df48.reset_index(drop=True)\n",
        "print(df48)\n",
        "print(df48['rating'].value_counts())\n",
        "df48.to_csv(r\"drive/My Drive/csv's/Worst/NBA20_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       1.0  Basically made to be a multiplayer game. To ma...\n",
            "1       1.0  Worst NBA 2K Ever ! Unfinished game with tonns...\n",
            "2       0.0  This is too much... you buy a game and it is o...\n",
            "3       0.0  A casino game with a basketball mini game? and...\n",
            "4       0.0  Trash casino game. Trash casino game. Trash ca...\n",
            "..      ...                                                ...\n",
            "552     0.0  It is incredibly disheartening to know 2K is w...\n",
            "553     0.0  Way too unrealistic and inconsistent, one game...\n",
            "554     0.0  Not user friendly, worst 2k so far , same neig...\n",
            "555     0.0  Un simulador más con todos sus juguetitos para...\n",
            "556     0.0  They ruin the threes.............................\n",
            "\n",
            "[557 rows x 2 columns]\n",
            "0.0     429\n",
            "1.0      43\n",
            "2.0      22\n",
            "10.0     18\n",
            "9.0      10\n",
            "5.0       8\n",
            "4.0       8\n",
            "8.0       7\n",
            "6.0       5\n",
            "3.0       4\n",
            "7.0       3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykUXisSFjxsD"
      },
      "source": [
        "# 3) FIFA 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOEzKcyhjyk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "357199f6-38a5-47a3-c4f9-c7c0c66ab3a8"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/fifa-20/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df49 = pd.DataFrame(review_dict)\n",
        "    df49 = df49.drop(columns=['name', 'date'])\n",
        "    df49.to_csv(r\"drive/My Drive/csv's/Worst/FIFA20_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(18), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydbL70uVjyzB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d3c5e906-086e-4a07-c026-50ca0efe49ae"
      },
      "source": [
        "df49 = pd.read_csv(\"drive/My Drive/csv's/Worst/FIFA20_reviews.csv\",engine='python' )\n",
        "df49 = df49.dropna()\n",
        "del df49['Unnamed: 0']\n",
        "df49 = df49.reset_index(drop=True)\n",
        "print(df49)\n",
        "print(df49['rating'].value_counts())\n",
        "df49.to_csv(r\"drive/My Drive/csv's/Worst/FIFA20_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        0.0  I see all these comments saying not to listen ...\n",
            "1        0.0  EA excels at every game manipulating games and...\n",
            "2        0.0  This game is totally **** Passes in this game ...\n",
            "3        0.0  This game hasn't changed since 2017! Just a fe...\n",
            "4        0.0  The defense changes of fifa 20 are absolutely ...\n",
            "...      ...                                                ...\n",
            "1621     0.0  Pokreti igraca su nekordinisani,fizika lopte k...\n",
            "1622     0.0  Haber si tenemos suerte y por fin desaparece e...\n",
            "1623    10.0  I think the grades is unfair with this game. I...\n",
            "1624     8.0  Игра,конечно не без проблем.Есть куча недорабо...\n",
            "1625     0.0  I was given this game for free by a friend. I ...\n",
            "\n",
            "[1626 rows x 2 columns]\n",
            "0.0     1164\n",
            "1.0      249\n",
            "10.0      45\n",
            "2.0       44\n",
            "3.0       31\n",
            "5.0       21\n",
            "8.0       18\n",
            "4.0       18\n",
            "6.0       16\n",
            "7.0       12\n",
            "9.0        8\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDOwMrm8jzEo"
      },
      "source": [
        "# 4) Star Wars Battlefront II (2017)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm39FTXsj1Ih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "7993b81a-e810-481a-9905-d1379e01fd52"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/star-wars-battlefront-ii/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df50 = pd.DataFrame(review_dict)\n",
        "    df50 = df50.drop(columns=['name', 'date'])\n",
        "    df50.to_csv(r\"drive/My Drive/csv's/Worst/BATTLEFRONTII_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(26), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEPfQicSj1UI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "2f13f7f8-9dc7-4cc3-ee34-790915e964d6"
      },
      "source": [
        "df50 = pd.read_csv(\"drive/My Drive/csv's/Worst/BATTLEFRONTII_reviews.csv\",engine='python' )\n",
        "df50 = df50.dropna()\n",
        "del df50['Unnamed: 0']\n",
        "df50 = df50.reset_index(drop=True)\n",
        "print(df50)\n",
        "print(df50['rating'].value_counts())\n",
        "df50.to_csv(r\"drive/My Drive/csv's/Worst/BATTLEFRONTII_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        1.0  PLS! ALL! Dont buy this game! If we continue t...\n",
            "1        0.0  - Preorder bonuses\\r- Tiered pricing (Standard...\n",
            "2        0.0  Thank you EA for the Microtransactions. Thank ...\n",
            "3        1.0  DICE did a pretty good job with the game in ov...\n",
            "4        0.0  Pay2Win is an absolute deal killer for me no m...\n",
            "...      ...                                                ...\n",
            "2374     7.0                       كل ما ذكرته من طور القصة فقط\n",
            "2375     0.0  Игра, безусловно, красивая, сюжетка не такая у...\n",
            "2376     2.0  The Game didn't even tried to be like the old ...\n",
            "2377     7.0  The story is ok.The gameplay is nice and addic...\n",
            "2378     6.0  A rocky start to the game, as I was hoping to ...\n",
            "\n",
            "[2379 rows x 2 columns]\n",
            "0.0     1694\n",
            "1.0      171\n",
            "10.0     132\n",
            "8.0       82\n",
            "9.0       75\n",
            "7.0       46\n",
            "3.0       45\n",
            "5.0       35\n",
            "2.0       34\n",
            "4.0       33\n",
            "6.0       32\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ml_fUaj1o4"
      },
      "source": [
        "# 5) Metal Gear Survive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FbKVdBdj2kA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6084a717-4e37-4e08-828b-650b533e42cc"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/metal-gear-survive/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df51 = pd.DataFrame(review_dict)\n",
        "    df51 = df51.drop(columns=['name', 'date'])\n",
        "    df51.to_csv(r\"drive/My Drive/csv's/Worst/MGS_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV3z3PPMj265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "8489a852-d111-4f17-e7cf-67ba746131a1"
      },
      "source": [
        "df51 = pd.read_csv(\"drive/My Drive/csv's/Worst/MGS_reviews.csv\",engine='python' )\n",
        "df51 = df51.dropna()\n",
        "del df51['Unnamed: 0']\n",
        "df51 = df51.reset_index(drop=True)\n",
        "print(df51)\n",
        "print(df51['rating'].value_counts())\n",
        "df51.to_csv(r\"drive/My Drive/csv's/Worst/MGS_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  Ripping gamers off! This might be the worst ga...\n",
            "1       0.0  Micro Transaction for basic game features?!?! ...\n",
            "2       0.0  \"This is like if Nintendo fired Miyamoto and a...\n",
            "3       3.0  This is not the worst game, however it is a re...\n",
            "4       0.0  Metal Gear Abortion - a cheap asset flip, game...\n",
            "..      ...                                                ...\n",
            "238    10.0                                                .at\n",
            "239     0.0  just another cash graber game with a lot of mi...\n",
            "240     6.0  I really wanted to like this game. I have play...\n",
            "241     0.0                                           F KONAMI\n",
            "242     5.0  Solo es un juego mas donde lo mas interesante ...\n",
            "\n",
            "[243 rows x 2 columns]\n",
            "0.0     159\n",
            "10.0     28\n",
            "1.0      14\n",
            "8.0      11\n",
            "7.0      10\n",
            "3.0       8\n",
            "2.0       4\n",
            "9.0       3\n",
            "6.0       3\n",
            "5.0       2\n",
            "4.0       1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvfCKO4uj3hw"
      },
      "source": [
        "# 6) Tony Hawk's Pro Skater 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx8vr5faj4lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7f9b1d96-68c5-477e-a99f-ee2fe47fdad4"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/tony-hawks-pro-skater-5/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df52 = pd.DataFrame(review_dict)\n",
        "    df52 = df52.drop(columns=['name', 'date'])\n",
        "    df52.to_csv(r\"drive/My Drive/csv's/Worst/THPS5_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XyjYQLyj8RB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "4310b3b4-4864-494f-91d3-3479670658b0"
      },
      "source": [
        "df52 = pd.read_csv(\"drive/My Drive/csv's/Worst/THPS5_reviews.csv\",engine='python' )\n",
        "df52 = df52.dropna()\n",
        "del df52['Unnamed: 0']\n",
        "df52 = df52.reset_index(drop=True)\n",
        "print(df52)\n",
        "print(df52['rating'].value_counts())\n",
        "df52.to_csv(r\"drive/My Drive/csv's/Worst/THPS5_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       1.0                            Robomodo strikes again!\n",
            "1       0.0  How Tony Hawk has fallen from grace. This game...\n",
            "2       0.0  Wait, so the fix patch for this game is bigger...\n",
            "3       1.0  It baffles me why major publishers don't do ex...\n",
            "4       0.0  This game is nothing but digital waste. How an...\n",
            "..      ...                                                ...\n",
            "111    10.0  Tony Hawk 5 is the latest and greatest install...\n",
            "112     3.0  When all is said and done, this game doesn’t d...\n",
            "113     0.0  Absolutely appalled to see how terrible this g...\n",
            "114     1.0  This game is the feces that is produced when s...\n",
            "115     2.0  Лучше бы и не пробовал играть. Графика вырви г...\n",
            "\n",
            "[116 rows x 2 columns]\n",
            "0.0     50\n",
            "1.0     20\n",
            "2.0     14\n",
            "3.0      6\n",
            "7.0      5\n",
            "8.0      4\n",
            "10.0     4\n",
            "6.0      4\n",
            "9.0      3\n",
            "5.0      3\n",
            "4.0      3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4EIZhaCj8iS"
      },
      "source": [
        "# 7) NFL 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61PjL1Ryj9UJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c807b9f5-9775-4e7d-ead5-48a439c8a24e"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/madden-nfl-20/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df53 = pd.DataFrame(review_dict)\n",
        "    df53 = df53.drop(columns=['name', 'date'])\n",
        "    df53.to_csv(r\"drive/My Drive/csv's/Worst/NFL20_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBrIasZZj9zA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "be0ebcf6-5c27-4ae3-8fbb-9b353ae9321a"
      },
      "source": [
        "df53 = pd.read_csv(\"drive/My Drive/csv's/Worst/NFL20_reviews.csv\",engine='python' )\n",
        "df53 = df53.dropna()\n",
        "del df53['Unnamed: 0']\n",
        "df53 = df53.reset_index(drop=True)\n",
        "print(df53)\n",
        "print(df53['rating'].value_counts())\n",
        "df53.to_csv(r\"drive/My Drive/csv's/Worst/NFL20_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       4.0  10 years ago I was dreaming of how amazing Mad...\n",
            "1       0.0  Grats EA... you hit the pinical of garbage... ...\n",
            "2       3.0  EA does the bare minimum to get people to thin...\n",
            "3       2.0  Face of franchise is just a stupid little 2 ga...\n",
            "4       0.0  Game regressed for the first time ever. Awful ...\n",
            "..      ...                                                ...\n",
            "313     0.0  Franchise mode is an exact copy and paste from...\n",
            "314     0.0                                #FixMaddenFranchise\n",
            "315     0.0  Fix your bleeping game, quit making excuses an...\n",
            "316     0.0  Dont buy this game. EA, for many years, has co...\n",
            "317     0.0  Full of glitches and no depth in franchise mod...\n",
            "\n",
            "[318 rows x 2 columns]\n",
            "0.0     195\n",
            "1.0      50\n",
            "2.0      21\n",
            "3.0      13\n",
            "8.0       9\n",
            "6.0       6\n",
            "4.0       6\n",
            "7.0       5\n",
            "10.0      5\n",
            "9.0       5\n",
            "5.0       3\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpDJDzidj-Mh"
      },
      "source": [
        "# 8) WWE 2K20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_pmNq03j-9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9f0d5b44-6e28-4870-a3aa-33f81f1aabb1"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/wwe-2k20/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df54 = pd.DataFrame(review_dict)\n",
        "    df54 = df54.drop(columns=['name', 'date'])\n",
        "    df54.to_csv(r\"drive/My Drive/csv's/Worst/WWE20_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytdj1oZzj_LZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "fea50b18-104c-4ec8-f3f5-6d328e4500ae"
      },
      "source": [
        "df54 = pd.read_csv(\"drive/My Drive/csv's/Worst/WWE20_reviews.csv\",engine='python' )\n",
        "df54 = df54.dropna()\n",
        "del df54['Unnamed: 0']\n",
        "df54 = df54.reset_index(drop=True)\n",
        "print(df54)\n",
        "print(df54['rating'].value_counts())\n",
        "df54.to_csv(r\"drive/My Drive/csv's/Worst/WWE20_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  I'm not surprised this game is a mess - Yukes ...\n",
            "1       0.0  This game is a huge step backwards. I love wre...\n",
            "2       0.0  Beyond broken. Only token things added to this...\n",
            "3       0.0  the worst game of the entire series, the game ...\n",
            "4       0.0  The game is literally unplayable. Tons of glit...\n",
            "..      ...                                                ...\n",
            "125     3.0  Игру не покупал, но мой друг Максим (кстати у ...\n",
            "126     2.0  In my opinion. I have played WWE games since 2...\n",
            "127     1.0  I've been playing Smackdown vs Raw and  their ...\n",
            "128     1.0  After an impressive showing with WWE 2K19, WWE...\n",
            "129     4.0  Sadly, Choosing to part ways with the old deve...\n",
            "\n",
            "[130 rows x 2 columns]\n",
            "0.0     69\n",
            "1.0     20\n",
            "2.0      9\n",
            "4.0      9\n",
            "8.0      5\n",
            "5.0      5\n",
            "10.0     4\n",
            "3.0      4\n",
            "9.0      2\n",
            "7.0      2\n",
            "6.0      1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWqAsXyKj_hQ"
      },
      "source": [
        "# 9) NBA 2K18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqAeG8i1kAkS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ac038521-c6b8-4205-8e76-9a024f4f93e3"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/nba-2k18/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df55 = pd.DataFrame(review_dict)\n",
        "    df55 = df55.drop(columns=['name', 'date'])\n",
        "    df55.to_csv(r\"drive/My Drive/csv's/Worst/NBA18_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LST2psL6kA0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "ebcba168-05c0-4d8c-c131-424ff8c9aea8"
      },
      "source": [
        "df55 = pd.read_csv(\"drive/My Drive/csv's/Worst/NBA18_reviews.csv\",engine='python' )\n",
        "df55 = df55.dropna()\n",
        "del df55['Unnamed: 0']\n",
        "df55 = df55.reset_index(drop=True)\n",
        "print(df55)\n",
        "print(df55['rating'].value_counts())\n",
        "df55.to_csv(r\"drive/My Drive/csv's/Worst/NBA18_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  Microtransactions don't belong in fully priced...\n",
            "1       0.0  Seriously, Im Canadian, i paid 79.99 + tax for...\n",
            "2       4.0  The last few years NBA 2K's were one thing but...\n",
            "3       1.0    You need a lot of VC to upgrade your attributes\n",
            "4       0.0  F2P game mechanics in a \"AAA\" title for 60$ fo...\n",
            "..      ...                                                ...\n",
            "235     0.0  Microtransactions!!\\rstay away from this infin...\n",
            "236     0.0  Damn. Critics are really getting paid to swall...\n",
            "237     0.0  Terrible game, no improvements and terrible an...\n",
            "238     2.0  Esse é o pior jogo de basquete que já joguei e...\n",
            "239     0.0  User ratings are always better than critic rat...\n",
            "\n",
            "[240 rows x 2 columns]\n",
            "0.0     134\n",
            "1.0      41\n",
            "2.0      12\n",
            "3.0      11\n",
            "4.0      10\n",
            "8.0       8\n",
            "5.0       7\n",
            "10.0      5\n",
            "6.0       4\n",
            "9.0       4\n",
            "7.0       4\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juiE_TockBSY"
      },
      "source": [
        "# 10) FIFA 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZdydJtQkCLQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3c3b7361-48b9-4dc2-8b91-01eefaa672ed"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/fifa-19/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df56 = pd.DataFrame(review_dict)\n",
        "    df56 = df56.drop(columns=['name', 'date'])\n",
        "    df56.to_csv(r\"drive/My Drive/csv's/Worst/FIFA19_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2kzEUGckCTg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d54b5ee4-1ff7-4aaf-ee8b-5bf902fb61a7"
      },
      "source": [
        "df56 = pd.read_csv(\"drive/My Drive/csv's/Worst/FIFA19_reviews.csv\",engine='python' )\n",
        "df56 = df56.dropna()\n",
        "del df56['Unnamed: 0']\n",
        "df56 = df56.reset_index(drop=True)\n",
        "print(df56)\n",
        "print(df56['rating'].value_counts())\n",
        "df56.to_csv(r\"drive/My Drive/csv's/Worst/FIFA19_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       1.0  if you drop your joystick the game play alone ...\n",
            "1       0.0    Fifa 19 Ping-pong edition with change colour...\n",
            "2       0.0  This game is trash.I don't know from where to ...\n",
            "3       0.0  Embarasing, nothing else, just embarasing, Emb...\n",
            "4       0.0  How could you possibly justify selling the exa...\n",
            "..      ...                                                ...\n",
            "359     3.0  There shouldn't be a percentage requirement to...\n",
            "360     2.0  The game is just like 18 and 17, the focus is ...\n",
            "361     0.0  O pior fifa que já joguei na minha vida, cheio...\n",
            "362     8.0  hhohohohoohohohhohohohohoohohohohohohohohohoho...\n",
            "363     5.0  التقييم ينطق ليس تقييمي فحسب بل تققيم الناس لل...\n",
            "\n",
            "[364 rows x 2 columns]\n",
            "0.0     202\n",
            "1.0      55\n",
            "2.0      20\n",
            "8.0      18\n",
            "10.0     14\n",
            "4.0      14\n",
            "5.0      12\n",
            "6.0       9\n",
            "3.0       9\n",
            "7.0       6\n",
            "9.0       5\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53D9-1GAkCnR"
      },
      "source": [
        "# 11) NFL 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8TNlJi9kDcy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "16c56eb1-8b4a-4009-c4c5-a4ec8f3add2d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/madden-nfl-19/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df57 = pd.DataFrame(review_dict)\n",
        "    df57 = df57.drop(columns=['name', 'date'])\n",
        "    df57.to_csv(r\"drive/My Drive/csv's/Worst/NFL19_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eQsOaTIkDk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "a8a1e040-eb19-4b3f-9974-04fc419d578a"
      },
      "source": [
        "df57 = pd.read_csv(\"drive/My Drive/csv's/Worst/NFL19_reviews.csv\",engine='python' )\n",
        "df57 = df57.dropna()\n",
        "del df57['Unnamed: 0']\n",
        "df57 = df57.reset_index(drop=True)\n",
        "print(df57)\n",
        "print(df57['rating'].value_counts())\n",
        "df57.to_csv(r\"drive/My Drive/csv's/Worst/NFL19_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       2.0  I'll state this right off the bat: I'm someone...\n",
            "1       0.0  Another game used by EA as a financial pawn to...\n",
            "2       0.0  I decided to get my old GameCube out and play ...\n",
            "3       4.0  Probably the last game I'll purchase from EA S...\n",
            "4       0.0  This game is simply trash. NFL doesn’t give a ...\n",
            "..      ...                                                ...\n",
            "117     0.0  I reset my ps4 because the game froze everythi...\n",
            "118     5.0  There is one single reason to buy this product...\n",
            "119     0.0  Absolutely garbage  plz do not buy.    I hope ...\n",
            "120     1.0  Just think copy and paste from previous years ...\n",
            "121     2.0  There charging AAA prices for a crap game. Mad...\n",
            "\n",
            "[122 rows x 2 columns]\n",
            "0.0     63\n",
            "2.0     16\n",
            "1.0     14\n",
            "4.0      8\n",
            "5.0      6\n",
            "8.0      3\n",
            "6.0      3\n",
            "3.0      3\n",
            "10.0     2\n",
            "9.0      2\n",
            "7.0      2\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrGpHLtVkD7Y"
      },
      "source": [
        "# 12) Battlefield V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxTfDzc5kEkI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "56cf7ce4-015a-4687-974d-d917663ddae5"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/battlefield-v/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df58 = pd.DataFrame(review_dict)\n",
        "    df58 = df58.drop(columns=['name', 'date'])\n",
        "    df58.to_csv(r\"drive/My Drive/csv's/Worst/BFV_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(9), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyrXEZlZkEzV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "346cbe39-320a-4687-8a24-fa006d5cb90f"
      },
      "source": [
        "df58 = pd.read_csv(\"drive/My Drive/csv's/Worst/BFV_reviews.csv\",engine='python' )\n",
        "df58 = df58.dropna()\n",
        "del df58['Unnamed: 0']\n",
        "df58 = df58.reset_index(drop=True)\n",
        "print(df58)\n",
        "print(df58['rating'].value_counts())\n",
        "df58.to_csv(r\"drive/My Drive/csv's/Worst/BFV_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  В игре про Вторую мировую войну нету страны, к...\n",
            "1       0.0  t has a lot of bugs, not completed story mode ...\n",
            "2       0.0  Лучше поиграть в Red Orchestra 2: Heroes of St...\n",
            "3       0.0  Waiting for EA to close DICE and they will sto...\n",
            "4       0.0  Developer said not to buy it.I totaly agree.It...\n",
            "..      ...                                                ...\n",
            "765     0.0  Стрельба в Хойке лучше чем в этом кале.Очень м...\n",
            "766     2.0  Worst battlefield ever hardline was bad but om...\n",
            "767     1.0  Как можно оценить игру в которой действия прои...\n",
            "768     4.0  As peculiaridades da Dice continuam ali. Quand...\n",
            "769     0.0  This game has its ups and downs. It is an alri...\n",
            "\n",
            "[770 rows x 2 columns]\n",
            "0.0     377\n",
            "1.0      93\n",
            "10.0     60\n",
            "2.0      41\n",
            "9.0      34\n",
            "5.0      33\n",
            "8.0      33\n",
            "4.0      29\n",
            "6.0      26\n",
            "7.0      23\n",
            "3.0      21\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leI85u_nkFRQ"
      },
      "source": [
        "# 13) NBA 2K19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htIS3PD6kGMR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "722ed13d-f068-4bd9-b715-7709542fc0af"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/nba-2k19/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df59 = pd.DataFrame(review_dict)\n",
        "    df59 = df59.drop(columns=['name', 'date'])\n",
        "    df59.to_csv(r\"drive/My Drive/csv's/Worst/NBA19_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHV8yevikGWY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "5e8a54db-a564-4716-d4d7-15ebde0a6690"
      },
      "source": [
        "df59 = pd.read_csv(\"drive/My Drive/csv's/Worst/NBA19_reviews.csv\",engine='python' )\n",
        "df59 = df59.dropna()\n",
        "del df59['Unnamed: 0']\n",
        "df59 = df59.reset_index(drop=True)\n",
        "print(df59)\n",
        "print(df59['rating'].value_counts())\n",
        "df59.to_csv(r\"drive/My Drive/csv's/Worst/NBA19_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       4.0  EDIT: Changing score to 6 after 3 full days of...\n",
            "1       1.0  I’ve been playing NBA 2K since 2k3 and I alway...\n",
            "2       0.0  Buggy mess full of microtransactions. Every ye...\n",
            "3       2.0  Paid for the 20th Year Anniversary edition and...\n",
            "4       5.0  Good: Seem to earn a bit more VC from games, H...\n",
            "..      ...                                                ...\n",
            "138    10.0  It's amazing to see how bad NBA 2K20 was for u...\n",
            "139     4.0  Está cada vez pior gostar de basquete e de vid...\n",
            "140     4.0  Servers are garbage as always. Ridiculous grin...\n",
            "141     0.0  I do not found nothing new in this \"new\" arcad...\n",
            "142     0.0  This thing is so scripted and so unfair when y...\n",
            "\n",
            "[143 rows x 2 columns]\n",
            "0.0     70\n",
            "1.0     24\n",
            "8.0     11\n",
            "4.0     10\n",
            "2.0      9\n",
            "10.0     7\n",
            "6.0      3\n",
            "3.0      3\n",
            "5.0      3\n",
            "9.0      2\n",
            "7.0      1\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Fh6f6dkG1o"
      },
      "source": [
        "# 14) Fallout 76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O38O3ihgkHyg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a9df7945-b4e3-4d31-82ba-1d3c161d5e42"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/fallout-76/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df60 = pd.DataFrame(review_dict)\n",
        "    df60 = df60.drop(columns=['name', 'date'])\n",
        "    df60.to_csv(r\"drive/My Drive/csv's/Worst/FALLOUT76_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(17), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh24sXDGkH8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "e18a324c-4fd7-4518-aa55-a5d48778fa32"
      },
      "source": [
        "df60 = pd.read_csv(\"drive/My Drive/csv's/Worst/FALLOUT76_reviews.csv\",engine='python' )\n",
        "df60 = df60.dropna()\n",
        "del df60['Unnamed: 0']\n",
        "df60 = df60.reset_index(drop=True)\n",
        "print(df60)\n",
        "print(df60['rating'].value_counts())\n",
        "df60.to_csv(r\"drive/My Drive/csv's/Worst/FALLOUT76_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        4.0  The team at Bethesda have developed an experie...\n",
            "1        0.0  My god, Bethesda... what have you done?\\rGraph...\n",
            "2        0.0  Today i fnished the first \"Dark Souls\" and jus...\n",
            "3        0.0  The franchise is no longer an RPG.  this game ...\n",
            "4        2.0  While it sounds interesting on paper, in pract...\n",
            "...      ...                                                ...\n",
            "1565     1.0  This game was In my opinion looked so cool on ...\n",
            "1566    10.0  Отличное ММО , длс с людьми , новые квесты и с...\n",
            "1567     0.0  Do i even need to say why?\\r It's fallout 76, ...\n",
            "1568     0.0  квесты не работают. Собрал кровь гуля, закинул...\n",
            "1569     7.0  The game is fun, and looks great. But still ha...\n",
            "\n",
            "[1570 rows x 2 columns]\n",
            "0.0     693\n",
            "10.0    186\n",
            "1.0     165\n",
            "9.0     115\n",
            "8.0     103\n",
            "2.0      95\n",
            "3.0      50\n",
            "7.0      47\n",
            "4.0      47\n",
            "5.0      40\n",
            "6.0      29\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYhzZ8bkkIWI"
      },
      "source": [
        "# 15) Modern Warfare (Extracted again for more recent reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urju5l9GkJCC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "84a37b0a-601f-48bb-c440-4a0934cb36d1"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/call-of-duty-modern-warfare/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df61 = pd.DataFrame(review_dict)\n",
        "    df61 = df61.drop(columns=['name', 'date'])\n",
        "    df61.to_csv(r\"drive/My Drive/csv's/Worst/MW_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(44), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n",
            "[*] Working on page: 9\n",
            "[*] Working on page: 10\n",
            "[*] Working on page: 11\n",
            "[*] Working on page: 12\n",
            "[*] Working on page: 13\n",
            "[*] Working on page: 14\n",
            "[*] Working on page: 15\n",
            "[*] Working on page: 16\n",
            "[*] Working on page: 17\n",
            "[*] Working on page: 18\n",
            "[*] Working on page: 19\n",
            "[*] Working on page: 20\n",
            "[*] Working on page: 21\n",
            "[*] Working on page: 22\n",
            "[*] Working on page: 23\n",
            "[*] Working on page: 24\n",
            "[*] Working on page: 25\n",
            "[*] Working on page: 26\n",
            "[*] Working on page: 27\n",
            "[*] Working on page: 28\n",
            "[*] Working on page: 29\n",
            "[*] Working on page: 30\n",
            "[*] Working on page: 31\n",
            "[*] Working on page: 32\n",
            "[*] Working on page: 33\n",
            "[*] Working on page: 34\n",
            "[*] Working on page: 35\n",
            "[*] Working on page: 36\n",
            "[*] Working on page: 37\n",
            "[*] Working on page: 38\n",
            "[*] Working on page: 39\n",
            "[*] Working on page: 40\n",
            "[*] Working on page: 41\n",
            "[*] Working on page: 42\n",
            "[*] Working on page: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGeZJhsbkJKo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "c71976fe-d25b-4a05-8701-18eae36a3075"
      },
      "source": [
        "df61 = pd.read_csv(\"drive/My Drive/csv's/Worst/MW_reviews.csv\",engine='python' )\n",
        "df61 = df61.dropna()\n",
        "del df61['Unnamed: 0']\n",
        "df61 = df61.reset_index(drop=True)\n",
        "print(df61)\n",
        "print(df61['rating'].value_counts())\n",
        "df61.to_csv(r\"drive/My Drive/csv's/Worst/MW_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      rating                                             review\n",
            "0        0.0  The developers just decided to accuse the Russ...\n",
            "1        0.0  This game is nothing but a russophobic propaga...\n",
            "2        0.0  I even created a Metacritic account for the re...\n",
            "3        0.0  This **** Propaganda. You think we buy this ga...\n",
            "4        0.0  Russophobic and hate-mongering game. Authors s...\n",
            "...      ...                                                ...\n",
            "3923     0.0  **** server for mp , lag comp , **** hit marke...\n",
            "3924     5.0  Would have totally loved to kill some russian!...\n",
            "3925     0.0  I never thought I’d ever give a game a zero. I...\n",
            "3926     8.0  when the game first came out it didn't win me ...\n",
            "3927     8.0  I thoroughly enjoyed this entry in the Call of...\n",
            "\n",
            "[3928 rows x 2 columns]\n",
            "0.0     2020\n",
            "10.0     814\n",
            "1.0      357\n",
            "9.0      185\n",
            "8.0      140\n",
            "2.0      121\n",
            "3.0       68\n",
            "7.0       67\n",
            "4.0       66\n",
            "6.0       47\n",
            "5.0       43\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvhHDmLFkJpZ"
      },
      "source": [
        "# 16) Anthem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6kDlSzHkKp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a666e847-2269-4217-ab45-f624705020dd"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/anthem/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df62 = pd.DataFrame(review_dict)\n",
        "    df62 = df62.drop(columns=['name', 'date'])\n",
        "    df62.to_csv(r\"drive/My Drive/csv's/Worst/ANTHEM_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO7jA8dgkK7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "33c81c85-d88f-4056-855b-27100d797843"
      },
      "source": [
        "df62 = pd.read_csv(\"drive/My Drive/csv's/Worst/ANTHEM_reviews.csv\",engine='python' )\n",
        "df62 = df62.dropna()\n",
        "del df62['Unnamed: 0']\n",
        "df62 = df62.reset_index(drop=True)\n",
        "print(df62)\n",
        "print(df62['rating'].value_counts())\n",
        "df62.to_csv(r\"drive/My Drive/csv's/Worst/ANTHEM_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       4.0  There is some fun to be had here but so many c...\n",
            "1       4.0  Its not a 0 but it is definitely not a 10 eith...\n",
            "2       0.0  12 hours into the game and the only thing Anth...\n",
            "3       1.0  One thing that won't be \"fixed in future\": the...\n",
            "4       3.0  jogo mal feito.vale no maximo 50 e pronto.EA s...\n",
            "..      ...                                                ...\n",
            "381     4.0  The game is missing a story line, or at least ...\n",
            "382     0.0  I expect that I can play a game for which I ha...\n",
            "383     0.0  This game has so much potential! It clearly fe...\n",
            "384     0.0  esse jogo quase queimou  meu ps4 , cheio de bu...\n",
            "385     0.0  Anthem is painful. Everything is so slow, so b...\n",
            "\n",
            "[386 rows x 2 columns]\n",
            "0.0     110\n",
            "10.0     60\n",
            "8.0      44\n",
            "2.0      31\n",
            "9.0      27\n",
            "3.0      27\n",
            "4.0      27\n",
            "1.0      26\n",
            "6.0      13\n",
            "7.0      12\n",
            "5.0       9\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HzNKI3TkLOI"
      },
      "source": [
        "# 17) FIFA 18 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMuY-1xwkMMI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6e8e6eeb-96e9-4039-8b17-f3da40495d72"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/fifa-18/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df63 = pd.DataFrame(review_dict)\n",
        "    df63 = df63.drop(columns=['name', 'date'])\n",
        "    df63.to_csv(r\"drive/My Drive/csv's/Worst/FIFA18_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEQnr6m_kMWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "ba239ca2-14c6-43d5-c68d-8defc8d97efb"
      },
      "source": [
        "df63 = pd.read_csv(\"drive/My Drive/csv's/Worst/FIFA18_reviews.csv\",engine='python' )\n",
        "df63 = df63.dropna()\n",
        "del df63['Unnamed: 0']\n",
        "df63 = df63.reset_index(drop=True)\n",
        "print(df63)\n",
        "print(df63['rating'].value_counts())\n",
        "df63.to_csv(r\"drive/My Drive/csv's/Worst/FIFA18_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  I must admit SCRIPTING is shamelessly there, y...\n",
            "1       0.0  loot boxes and pay to win do not buy keep your...\n",
            "2       0.0  Сделать честную игру!Это не киберспорт!!!Сообщ...\n",
            "3       0.0  Mas de lo mismo una y otra vez sacando dinero ...\n",
            "4       0.0  nu recomand acest joc , e prea ofticat :)))) o...\n",
            "..      ...                                                ...\n",
            "178     6.0  Gráficos decentes, es agradable con amigos, no...\n",
            "179     7.0  Igual que los demás, pero un poco mejor, los s...\n",
            "180     7.0  7/10------------------------------------------...\n",
            "181     0.0  They make a **** of optimization for this game...\n",
            "182     4.0                         لعبة حتى القرود تأبى لعبها\n",
            "\n",
            "[183 rows x 2 columns]\n",
            "0.0     59\n",
            "8.0     27\n",
            "1.0     18\n",
            "10.0    15\n",
            "6.0     14\n",
            "7.0     12\n",
            "5.0      9\n",
            "4.0      9\n",
            "9.0      8\n",
            "3.0      8\n",
            "2.0      4\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpV0ziyQkMpp"
      },
      "source": [
        "# 18) Street Fighter V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPzR7kmrkNtQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "16280676-fdf1-4aaa-ebb5-645b1e19d1b1"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/street-fighter-v/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df64 = pd.DataFrame(review_dict)\n",
        "    df64 = df64.drop(columns=['name', 'date'])\n",
        "    df64.to_csv(r\"drive/My Drive/csv's/Worst/SFV_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(6), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3sLIuZtkN7o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "e0e00c33-2e4b-420e-86ed-8bce4b0ecd3b"
      },
      "source": [
        "df64 = pd.read_csv(\"drive/My Drive/csv's/Worst/SFV_reviews.csv\",engine='python' )\n",
        "df64 = df64.dropna()\n",
        "del df64['Unnamed: 0']\n",
        "df64 = df64.reset_index(drop=True)\n",
        "print(df64)\n",
        "print(df64['rating'].value_counts())\n",
        "df64.to_csv(r\"drive/My Drive/csv's/Worst/SFV_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       2.0  The game is not completed. Feels like its a de...\n",
            "1       2.0  sad sad sad... Street Fighter doesnt deserve t...\n",
            "2       2.0  I stayed up until the midnight release... I ha...\n",
            "3       2.0  most multiplayer games come out like this. Alw...\n",
            "4       0.0  English is not my native language , but that d...\n",
            "..      ...                                                ...\n",
            "442     0.0  So, you literally just buy game but, you dont ...\n",
            "443     0.0  La más grande estafa de los últimos 20 años. N...\n",
            "444     1.0  Increible que capcom venda un juego incompleto...\n",
            "445     7.0  A terrible launch, but a great improvement ove...\n",
            "446     6.0  Look. The game isn't as terrible as everyone s...\n",
            "\n",
            "[447 rows x 2 columns]\n",
            "0.0     144\n",
            "1.0      60\n",
            "10.0     42\n",
            "4.0      38\n",
            "2.0      34\n",
            "3.0      29\n",
            "8.0      23\n",
            "6.0      23\n",
            "9.0      22\n",
            "5.0      17\n",
            "7.0      15\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cmzmpUtkOUQ"
      },
      "source": [
        "# 19) Fortnite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fev5vHVwkPxA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2ba4dc98-afc5-4843-f9e5-1d3dadd157df"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/fortnite/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df65 = pd.DataFrame(review_dict)\n",
        "    df65 = df65.drop(columns=['name', 'date'])\n",
        "    df65.to_csv(r\"drive/My Drive/csv's/Worst/FORTNITE_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6he1mnckP5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d0a23309-e4e2-418b-aa8f-5e35bd2a3a98"
      },
      "source": [
        "df65 = pd.read_csv(\"drive/My Drive/csv's/Worst/FORTNITE_reviews.csv\",engine='python' )\n",
        "df65 = df65.dropna()\n",
        "del df65['Unnamed: 0']\n",
        "df65 = df65.reset_index(drop=True)\n",
        "print(df65)\n",
        "print(df65['rating'].value_counts())\n",
        "df65.to_csv(r\"drive/My Drive/csv's/Worst/FORTNITE_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       1.0  Short: Save your money!\\r This game has Potent...\n",
            "1       1.0  I can't recommend this game and the behavior o...\n",
            "2       0.0                                 Pay to Pay to win.\n",
            "3      10.0  Super Addictive Game. Played more than 20 Hour...\n",
            "4       5.0  Gameplay is solid.  Was having lots of fun.  K...\n",
            "..      ...                                                ...\n",
            "271    10.0  este jogo é muito bom, foi o melhor jogo onlin...\n",
            "272     0.0  TRASH Same **** every round, only little 7 yea...\n",
            "273     0.0  this game is the worst game in the history of ...\n",
            "274     1.0              Gramnda didn gave me gaem for crimmas\n",
            "275     0.0  Einfach gesagt das spiel ist Scheiße schlechte...\n",
            "\n",
            "[276 rows x 2 columns]\n",
            "0.0     111\n",
            "1.0      25\n",
            "10.0     21\n",
            "8.0      20\n",
            "9.0      18\n",
            "6.0      15\n",
            "4.0      15\n",
            "5.0      14\n",
            "2.0      13\n",
            "7.0      12\n",
            "3.0      12\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO8miYNhkQRw"
      },
      "source": [
        "# 20) Mortal Kombat 11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf5FZJlukRLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e0b3ddb6-19b1-4b7c-aa2d-aa55d38be9d7"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/mortal-kombat-11/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df66 = pd.DataFrame(review_dict)\n",
        "    df66 = df66.drop(columns=['name', 'date'])\n",
        "    df66.to_csv(r\"drive/My Drive/csv's/Worst/MK11_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(9), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n",
            "[*] Working on page: 7\n",
            "[*] Working on page: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xChKtqgkRVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "e88046dd-94f3-43a3-bde1-c25ea49b02ea"
      },
      "source": [
        "df66 = pd.read_csv(\"drive/My Drive/csv's/Worst/MK11_reviews.csv\",engine='python' )\n",
        "df66 = df66.dropna()\n",
        "del df66['Unnamed: 0']\n",
        "df66 = df66.reset_index(drop=True)\n",
        "print(df66)\n",
        "print(df66['rating'].value_counts())\n",
        "df66.to_csv(r\"drive/My Drive/csv's/Worst/MK11_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       1.0  Оптимизации нет, добавили кучу ненужного донат...\n",
            "1       0.0  Always online single player game with microSca...\n",
            "2       4.0  You done **** it up ! What the hell ? All this...\n",
            "3       0.0  The game is ok. Good graphics, sound and gamep...\n",
            "4       0.0  6000$ for unlocking all? Hahahah no, thanks, W...\n",
            "..      ...                                                ...\n",
            "745     7.0  First, the good aspects of this game: Mortal K...\n",
            "746     9.0  As of now, this is peak Mortal Kombat, and lik...\n",
            "747     6.0  In my opinion this is the worst mortal kombat ...\n",
            "748     9.0  Şu zamana kadar çıkmış en iyi ''Mortal Kombat'...\n",
            "749     7.0  Una buena historia, buenos personajes, buena j...\n",
            "\n",
            "[750 rows x 2 columns]\n",
            "0.0     302\n",
            "10.0    101\n",
            "1.0      90\n",
            "9.0      51\n",
            "2.0      50\n",
            "8.0      35\n",
            "3.0      35\n",
            "4.0      31\n",
            "5.0      20\n",
            "6.0      19\n",
            "7.0      16\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnP_S7_CkRlP"
      },
      "source": [
        "# 21) Call of Duty Ghosts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF2TXKJekSsg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a94048f1-01f7-4bea-e010-5aca4a4bcc93"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/call-of-duty-ghosts/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df67 = pd.DataFrame(review_dict)\n",
        "    df67 = df67.drop(columns=['name', 'date'])\n",
        "    df67.to_csv(r\"drive/My Drive/csv's/Worst/GHOSTS_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZwMej5FkS6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "53f3845e-727c-4011-edb5-b91fb8030df4"
      },
      "source": [
        "df67 = pd.read_csv(\"drive/My Drive/csv's/Worst/GHOSTS_reviews.csv\",engine='python' )\n",
        "df67 = df67.dropna()\n",
        "del df67['Unnamed: 0']\n",
        "df67 = df67.reset_index(drop=True)\n",
        "print(df67)\n",
        "print(df67['rating'].value_counts())\n",
        "df67.to_csv(r\"drive/My Drive/csv's/Worst/GHOSTS_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       5.0  Why can't I learn my lesson? Every year its th...\n",
            "1       2.0  The sad truth is, Call of Duty campaigns  and ...\n",
            "2       1.0  If you play any Call of Duty, you've played GH...\n",
            "3       2.0  rinse and repeat vol. 3000 so rinse and repeat...\n",
            "4       1.0  Been a CoD for since CoD2 (not MW2). This game...\n",
            "..      ...                                                ...\n",
            "344     3.0  Bored, generic, repetitive, a bad story and a ...\n",
            "345     6.0  Ghosts isn't a bad game; it just isn't special...\n",
            "346     9.0  This game is an Activision game and i can say ...\n",
            "347     7.0  Tiene muy mala fama pero para mi es uno de los...\n",
            "348    10.0  fjejdukkekxxekkxnexbezgwyasnswxkuexyzvgwzbqtbz...\n",
            "\n",
            "[349 rows x 2 columns]\n",
            "0.0     61\n",
            "10.0    43\n",
            "8.0     36\n",
            "6.0     30\n",
            "7.0     30\n",
            "5.0     30\n",
            "4.0     28\n",
            "2.0     25\n",
            "3.0     22\n",
            "9.0     22\n",
            "1.0     22\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7WR_0T2kTL4"
      },
      "source": [
        "# 22) Call of Duty: Infinite Warfare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIGaMC7DkUGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "98be821e-a8ca-4bf7-db9f-1523d23d60bb"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/call-of-duty-infinite-warfare/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df68 = pd.DataFrame(review_dict)\n",
        "    df68 = df68.drop(columns=['name', 'date'])\n",
        "    df68.to_csv(r\"drive/My Drive/csv's/Worst/IW_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(5), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q-nxLqQkUPQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "d9b39979-2555-4d5d-9fe8-294e398dab6f"
      },
      "source": [
        "df68 = pd.read_csv(\"drive/My Drive/csv's/Worst/IW_reviews.csv\",engine='python' )\n",
        "df68 = df68.dropna()\n",
        "del df68['Unnamed: 0']\n",
        "df68 = df68.reset_index(drop=True)\n",
        "print(df68)\n",
        "print(df68['rating'].value_counts())\n",
        "df68.to_csv(r\"drive/My Drive/csv's/Worst/IW_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       5.0  Campaign is okay. JUST okay. I liked the visua...\n",
            "1       6.0  Here's the deal. COD IW is not a bad game. But...\n",
            "2       7.0  Story/Campaign: Thrilling dogfights, excellent...\n",
            "3       3.0  I couldn't be more livid. The lad who said thi...\n",
            "4       0.0  Shameless copy paste of blops 3, once more wit...\n",
            "..      ...                                                ...\n",
            "376    10.0  I will note cod iw on one factor, the campaign...\n",
            "377     6.0  El peor cod que jugue, repetitivo como siempre...\n",
            "378     3.0  I know this game gets alot of hate, and I can ...\n",
            "379    10.0  Hands down this is the best campaign out of an...\n",
            "380     8.0  El juego en sí es muy bueno, pero tiene sus pe...\n",
            "\n",
            "[381 rows x 2 columns]\n",
            "0.0     85\n",
            "10.0    65\n",
            "8.0     40\n",
            "5.0     31\n",
            "1.0     30\n",
            "9.0     28\n",
            "7.0     25\n",
            "4.0     22\n",
            "3.0     22\n",
            "6.0     17\n",
            "2.0     16\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD_niBeTkUlH"
      },
      "source": [
        "# 23) Call of Duty: Black Ops 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72vKYXG7kVVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "b845fcff-1549-4733-a193-209301580f5d"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/call-of-duty-black-ops-4/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df69 = pd.DataFrame(review_dict)\n",
        "    df69 = df69.drop(columns=['name', 'date'])\n",
        "    df69.to_csv(r\"drive/My Drive/csv's/Worst/BO4_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(7), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n",
            "[*] Working on page: 4\n",
            "[*] Working on page: 5\n",
            "[*] Working on page: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLyorAhGkVf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "1530b597-8e03-432a-e99f-2a5f208b257d"
      },
      "source": [
        "df69 = pd.read_csv(\"drive/My Drive/csv's/Worst/BO4_reviews.csv\",engine='python' )\n",
        "df69 = df69.dropna()\n",
        "del df69['Unnamed: 0']\n",
        "df69 = df69.reset_index(drop=True)\n",
        "print(df69)\n",
        "print(df69['rating'].value_counts())\n",
        "df69.to_csv(r\"drive/My Drive/csv's/Worst/BO4_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       4.0  The removal of a single-player campaign is ind...\n",
            "1       0.0  No campaign, garbage and a waste of my time. T...\n",
            "2       0.0  Ever notice why Infinity Ward games had a bett...\n",
            "3       1.0  Why are people who praise this game taking sho...\n",
            "4       2.0  COD BO has always has the best COD campaigns. ...\n",
            "..      ...                                                ...\n",
            "499     6.0  Un buen juego que ha logrado recuperar mi inte...\n",
            "500     6.0  Would be higher if the micro transaction syste...\n",
            "501     0.0  This game is garbage. Terribly long, and borin...\n",
            "502     4.0  I hated this game initially, but overtime, i s...\n",
            "503     7.0  عبث باللاعبين واستغلال حبهم لسلسلة من أجل حلبه...\n",
            "\n",
            "[504 rows x 2 columns]\n",
            "0.0     158\n",
            "10.0     68\n",
            "1.0      51\n",
            "8.0      43\n",
            "2.0      32\n",
            "9.0      30\n",
            "7.0      29\n",
            "5.0      28\n",
            "3.0      27\n",
            "4.0      20\n",
            "6.0      18\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOSRwP_CkVtn"
      },
      "source": [
        "# 24) Far Cry: New Dawn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez266isDkWwp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "8a9b70ac-706c-4293-c5d6-27f02a4a86c2"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/far-cry-new-dawn/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df70 = pd.DataFrame(review_dict)\n",
        "    df70 = df70.drop(columns=['name', 'date'])\n",
        "    df70.to_csv(r\"drive/My Drive/csv's/Worst/FCND_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(3), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzg_--kHkW50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "67551929-91a6-4877-c85f-7705d8d9b8b4"
      },
      "source": [
        "df70 = pd.read_csv(\"drive/My Drive/csv's/Worst/FCND_reviews.csv\",engine='python' )\n",
        "df70 = df70.dropna()\n",
        "del df70['Unnamed: 0']\n",
        "df70 = df70.reset_index(drop=True)\n",
        "print(df70)\n",
        "print(df70['rating'].value_counts())\n",
        "df70.to_csv(r\"drive/My Drive/csv's/Worst/FCND_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       0.0  Easy cash-in for Ubisoft before the inevitable...\n",
            "1       0.0  Re-used Far Cry 5 locations with sunny-rainbow...\n",
            "2       2.0  This production is a really shame...is trash.....\n",
            "3       0.0  Это глупая, отвратительная игра на базе дальни...\n",
            "4       0.0  Whoever is directing this train-wreck needs to...\n",
            "..      ...                                                ...\n",
            "139     8.0  One of the action games which try to be more R...\n",
            "140     0.0  В начале игры нету возможности играть по стелс...\n",
            "141     7.0  Not the best Far Cry but still slightly better...\n",
            "142     8.0  As long as you understand what you're getting ...\n",
            "143     0.0  Uma porcaria, os mesmos bots bugs do Far Cry 5...\n",
            "\n",
            "[144 rows x 2 columns]\n",
            "0.0     36\n",
            "10.0    22\n",
            "8.0     19\n",
            "7.0     15\n",
            "2.0     10\n",
            "4.0      9\n",
            "6.0      9\n",
            "9.0      8\n",
            "1.0      6\n",
            "3.0      6\n",
            "5.0      4\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd5Js-RMkXNI"
      },
      "source": [
        "# 25) Need for Speed: Payback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVF1zitkYR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7410a3d5-ff94-478a-ff02-21cd640b254f"
      },
      "source": [
        "def scrap_page(page, review_dict):\n",
        "    url = 'https://www.metacritic.com/game/playstation-4/need-for-speed-payback/user-reviews?page=' + str(page)\n",
        "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=user_agent)\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for review in soup.find_all('div', class_='review_content'):\n",
        "        if review.find('div', class_='name') == None:\n",
        "            break\n",
        "        review_dict['name'].append(review.find('div', class_='name').find('a').text)\n",
        "        review_dict['date'].append(review.find('div', class_='date').text)\n",
        "        review_dict['rating'].append(review.find('div', class_='review_grade').find_all('div')[0].text)\n",
        "        if review.find('span', class_='blurb blurb_expanded'):\n",
        "            review_dict['review'].append(\n",
        "                review.find('span', class_='blurb blurb_expanded').text if review.find('span',\n",
        "                                                                                       class_='blurb blurb_expanded') else None)\n",
        "        else:\n",
        "            review_dict['review'].append(\n",
        "                review.find('div', class_='review_body').find('span').text if review.find('div',\n",
        "                                                                                          class_='review_body').find(\n",
        "                    'span') else None)\n",
        "\n",
        "\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    review_dict = {'name': [], 'date': [], 'rating': [], 'review': []}\n",
        "    for i in out:\n",
        "        thread_list = []\n",
        "        for page in i:\n",
        "            print('[*] Working on page: %d' % page)\n",
        "            thread = threading.Thread(target=scrap_page, args=(page, review_dict,))\n",
        "            thread_list.append(thread)\n",
        "            thread.start()\n",
        "        for t in thread_list:\n",
        "            t.join()\n",
        "    df71 = pd.DataFrame(review_dict)\n",
        "    df71 = df71.drop(columns=['name', 'date'])\n",
        "    df71.to_csv(r\"drive/My Drive/csv's/Worst/NFSP_reviews.csv\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    chunkIt(range(4), 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Working on page: 0\n",
            "[*] Working on page: 1\n",
            "[*] Working on page: 2\n",
            "[*] Working on page: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx7jlz_nkYao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "75514980-e4a4-47cc-b132-8f7c87ab4171"
      },
      "source": [
        "df71 = pd.read_csv(\"drive/My Drive/csv's/Worst/NFSP_reviews.csv\",engine='python' )\n",
        "df71 = df71.dropna()\n",
        "del df71['Unnamed: 0']\n",
        "df71 = df71.reset_index(drop=True)\n",
        "print(df71)\n",
        "print(df71['rating'].value_counts())\n",
        "df71.to_csv(r\"drive/My Drive/csv's/Worst/NFSP_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     rating                                             review\n",
            "0       1.0  EA ruins another game with a loot box progress...\n",
            "1       4.0  Its not the worst entry, however its obvious t...\n",
            "2       0.0  Just disappointed EA. Pumping the machine with...\n",
            "3       0.0  Another lootboxes title which gives you 2 choi...\n",
            "4       0.0  The cool stuff, like the fast & furious kind o...\n",
            "..      ...                                                ...\n",
            "222     8.0  Вполне адекватный NFS, к управлению привыкаешь...\n",
            "223     2.0  EA is a disaster of gaming with their lutbox, ...\n",
            "224     2.0  Nothing to say more about than the gameplay is...\n",
            "225     2.0  Interesting story. Well, interesting enough fo...\n",
            "226     1.0  Very very bad. Needfor speed 2015 was better t...\n",
            "\n",
            "[227 rows x 2 columns]\n",
            "0.0     85\n",
            "1.0     25\n",
            "10.0    23\n",
            "8.0     18\n",
            "9.0     16\n",
            "2.0     14\n",
            "4.0     14\n",
            "3.0     11\n",
            "7.0      8\n",
            "5.0      7\n",
            "6.0      6\n",
            "Name: rating, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}